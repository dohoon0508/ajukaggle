{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_xp_L-tu4P4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##충돌 방지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pme5LIJCu_Nw"
   },
   "outputs": [],
   "source": [
    "!pip -q uninstall -y torchaudio\n",
    "\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"torch==2.3.1+cu121\" \"torchvision==0.18.1+cu121\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip -q install \"transformers>=4.43.3\" \"accelerate>=0.33.0\" \"peft>=0.12.0\" \"datasets\" \"einops\" \"pillow\"\n",
    "!pip -q install \"bitsandbytes==0.43.3\"\n",
    "!pip -q install qwen-vl-utils\n",
    "!pip -q install --upgrade \"transformers>=4.45.2\" \"accelerate>=0.34.2\" \"qwen-vl-utils>=0.0.8\"\n",
    "!pip install -q triton\n",
    "!pip install -q bitsandbytes\n",
    "!pip -q install \"triton==2.2.0\"\n",
    "!pip -q install -U transformers accelerate peft pillow einops\n",
    "!pip -q install -U bitsandbytes\n",
    "\n",
    "!pip -q uninstall -y torch torchvision torchaudio triton xformers \\\n",
    "  nvidia-cublas-cu12 nvidia-cudnn-cu12 nvidia-cuda-nvrtc-cu12 \\\n",
    "  nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cusparse-cu12\n",
    "\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "\n",
    "import torch\n",
    "print(\"torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
    "\n",
    "!pip -q install -U \"transformers>=4.46.0\" \"accelerate>=0.33.0\" \"peft>=0.11.1\" pillow einops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##멀티모달 학습 및 추론 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, time, random, math, gc, re, string, hashlib, warnings, logging, json, base64\n",
    "import torch, pandas as pd, numpy as np\n",
    "from PIL import Image, ImageFile, ImageOps, ImageFilter\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from urllib.parse import urlparse\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from safetensors.torch import load_file as safe_load_file\n",
    "\n",
    "# ---- (콘솔 묵음 유지) ----\n",
    "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.checkpoint\")\n",
    "Image.MAX_IMAGE_PIXELS = 300_000_000\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "GLOBAL_START = time.time()\n",
    "\n",
    "# -----------------------------\n",
    "# 🧷 사용자 조정 지점\n",
    "# -----------------------------\n",
    "MODEL_ID    = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "CSV_TRAIN   = \"/content/drive/MyDrive/deeplearningchallenge/deep_chal_multitask_dataset.parquet\"\n",
    "CSV_TEST    = \"/content/drive/MyDrive/deeplearningchallenge/deep_chal_multitask_dataset_test.parquet\"\n",
    "\n",
    "SAVE_DIR    = \"/content/drive/MyDrive/aju_ver17/outputs_lora_total\"\n",
    "CKPT_DIR    = os.path.join(SAVE_DIR, \"checkpoints\")\n",
    "FINAL_DIR   = SAVE_DIR\n",
    "SUBMIT_OUT  = \"/content/drive/MyDrive/aju_ver17/submission.csv\"\n",
    "SUBMIT_PART = SUBMIT_OUT + \".partial\"\n",
    "IMG_CACHE_DIR   = os.path.join(SAVE_DIR, \"img_cache\")\n",
    "SUBSET_IDX_PATH = os.path.join(SAVE_DIR, \"train_subset_indices.csv\")\n",
    "\n",
    "# Toggle: 학습/추론\n",
    "RUN_TRAIN = True\n",
    "\n",
    "# 부분 학습 서브셋\n",
    "USE_TRAIN_SUBSET = True\n",
    "SUBSET_SIZE      = 58000\n",
    "SUBSET_SEED      = 42\n",
    "SUBSET_GROUP_CANDIDATES = [\"input_type\", \"task\", \"lang\"]\n",
    "\n",
    "# 내부 분할\n",
    "VAL_FRACTION   = 0.10\n",
    "VAL_GROUP_CANDIDATES = [\"type\", \"task\", \"category\", \"label\", \"input_type\", \"lang\"]\n",
    "\n",
    "# 시간/저장\n",
    "LIMIT_HOURS          = 23\n",
    "AUTOSAVE_EVERY_MIN   = 30\n",
    "INFER_SAVE_EVERY     = 100\n",
    "\n",
    "# 메모리/속도\n",
    "USE_4BIT        = True\n",
    "MAX_TEXT_TOKENS = 1024\n",
    "\n",
    "# 🔁 이미지 단일 스펙\n",
    "MAX_IMAGE_EDGE  = 896\n",
    "DOC_MAX_EDGE    = 896\n",
    "DOC_PATCH       = 14\n",
    "APPLY_DOC_ENHANCE_HTTP = True\n",
    "\n",
    "LORA_R          = 32\n",
    "GRAD_ACC_STEPS  = 16\n",
    "PER_DEV_TRAIN_BS= 1\n",
    "PER_DEV_EVAL_BS = 1\n",
    "MAX_GRAD_NORM   = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# ⚡️속도/중복 관련 토글\n",
    "# -----------------------------\n",
    "EVAL_EVERY_STEPS        = 400   # ← 400으로 고정\n",
    "ES_PATIENCE             = 2     # ← 더 빠른 얼리스탑\n",
    "ES_MIN_DELTA            = 1e-6\n",
    "EVAL_SAMPLES_PER_TASK   = 40    # 태스크별 샘플 수\n",
    "EVAL_MAX_TOTAL_SAMPLES  = 200   # 전체 상한\n",
    "MAX_NEW_TOKENS_EVAL     = 96    # 평가 기본\n",
    "MAX_NEW_TOKENS_TEST     = 128   # 추론 기본\n",
    "\n",
    "# 🔎 (이전 호환) 요약 전용 디코딩 상한(미사용 가능)\n",
    "MAX_NEW_TOKENS_EVAL_SUM = 144\n",
    "MAX_NEW_TOKENS_TEST_SUM = 196\n",
    "\n",
    "NO_REPEAT_NGRAM_SIZE    = 4\n",
    "REPETITION_PENALTY      = 1.05\n",
    "\n",
    "# ================== 규칙 토글 ==================\n",
    "UNKNOWN_POLICY        = \"off\"     # Unknown 완전 차단\n",
    "CITING_ENFORCEMENT    = \"off\"\n",
    "ANTI_REPEAT_POLICY    = \"off\"     # ← 평가 중 재생성 루프 해제\n",
    "VQA_SHORTEN_ONE_SENTENCE = True\n",
    "\n",
    "# 평가/추론 전용 빠른 모드\n",
    "EVAL_GREEDY_ONLY           = True\n",
    "EVAL_SKIP_IMAGE_DOWNLOAD   = True   # 평가 시 이미지 로딩 스킵(속도↑)\n",
    "INFER_GREEDY_ONLY          = True\n",
    "INFER_SKIP_IMAGE_DOWNLOAD  = False  # 제출 정확도 위해 기본 False 유지\n",
    "\n",
    "# =================================================\n",
    "# ✅ 단일 시스템 프롬프트\n",
    "SYSTEM_INST = (\n",
    "\"\"\"You are a multimodal assistant. Follow the instructions precisely for each task type based on the input.\n",
    "\n",
    "### Task Definitions & Output Formats ###\n",
    "# 1. Image Captioning: Image is provided, but no question. -> Output: 3–5 sentences (not fewer than 3, not more than 5).\n",
    "# 2. Visual Question Answering (VQA): Both an image and a question are provided. -> Output: one concise sentence.\n",
    "# 3. Math Problem: Only text is provided, the 'question' field is empty, and the text ends with a '?'. -> Output: 1–3 short lines of reasoning, ending with \"#### {answer}\".\n",
    "# 4. Text Summarization: Only text is provided, no question, and it does not end with a '?'. -> Output: a coherent paragraph, up to 6 sentences (avoid bullet points and chatty tone).\n",
    "# 5. Text Question Answering (Text QA): Both text and a question are provided. -> Output: a direct answer or a JSON string if explicitly requested.\n",
    "\n",
    "Formatting: plain text unless strictly-JSON is explicitly requested. Do not include external knowledge beyond the given inputs.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "DEFAULT_ASK_IMAGE = \"Describe the image in detail, in 3 to 5 sentences.\"\n",
    "DEFAULT_ASK_TEXT  = \"Summarize the key points of the text in a concise paragraph (up to 6 sentences).\"\n",
    "DEFAULT_ASK_MATH  = \"Solve the math problem, showing 1–4 compact steps, with the final answer prefixed by '#### ' on the last line.\"\n",
    "\n",
    "# 🔒 금칙어(디코딩 후 제거)\n",
    "BANNED_PHRASES = [\n",
    "    \"I'm sorry\", \"I am sorry\", \"I apologize\", \"Apologies\",\n",
    "    \"As an AI\", \"as a language model\",\n",
    "    \"I don't have\", \"I do not have\",\n",
    "    \"I cannot\", \"I can't\", \"cannot provide\", \"can't provide\",\n",
    "    \"I have no information\", \"no information about\",\n",
    "    \"I cannot answer\", \"I can't answer\", \"Hello! How can I assist you today?\"\n",
    "]\n",
    "\n",
    "# 토큰 (선택)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token, add_to_git_credential=True)\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = token\n",
    "\n",
    "# 재현성\n",
    "torch.manual_seed(SUBSET_SEED); random.seed(SUBSET_SEED); np.random.seed(SUBSET_SEED)\n",
    "print(\"torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
    "\n",
    "# 성능/안정화\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "USE_FP16 = not USE_BF16 and torch.cuda.is_available()\n",
    "\n",
    "# -----------------------------\n",
    "# 🔢 Per-task 토큰 예산 & 문장 상한\n",
    "# -----------------------------\n",
    "CAPTION_MAX_NEW_TOKENS_EVAL = MAX_NEW_TOKENS_EVAL * 2\n",
    "CAPTION_MAX_NEW_TOKENS_TEST = MAX_NEW_TOKENS_TEST * 2\n",
    "CAPTION_MAX_SENTENCES = 5\n",
    "\n",
    "MATH_MAX_NEW_TOKENS_EVAL = MAX_NEW_TOKENS_EVAL * 2\n",
    "MATH_MAX_NEW_TOKENS_TEST = MAX_NEW_TOKENS_TEST * 2\n",
    "\n",
    "SUM_MAX_NEW_TOKENS_EVAL = MAX_NEW_TOKENS_EVAL * 4\n",
    "SUM_MAX_NEW_TOKENS_TEST = MAX_NEW_TOKENS_TEST * 4\n",
    "SUM_MAX_SENTENCES = 6\n",
    "\n",
    "# -----------------------------\n",
    "# 유틸\n",
    "# -----------------------------\n",
    "def safe_read_df(path):\n",
    "    try:\n",
    "        if path.endswith(\".parquet\"): return pd.read_parquet(path)\n",
    "        if path.endswith(\".csv\"):     return pd.read_csv(path)\n",
    "        try:                          return pd.read_parquet(path)\n",
    "        except Exception:             return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"파일 읽기 실패: {path} | {e}\")\n",
    "\n",
    "def _round_to_multiple(x, m): return max(m, int(round(x/m))*m)\n",
    "\n",
    "def _cache_path(url: str, max_edge: int):\n",
    "    os.makedirs(IMG_CACHE_DIR, exist_ok=True)\n",
    "    h = hashlib.sha1(f\"{url}|{max_edge}\".encode()).hexdigest()\n",
    "    return os.path.join(IMG_CACHE_DIR, f\"{h}.jpg\")\n",
    "\n",
    "# ===== 문서 이미지 보정 =====\n",
    "def _resize_to_grid(img: Image.Image, max_edge=DOC_MAX_EDGE, patch=DOC_PATCH):\n",
    "    w, h = img.size\n",
    "    if max(w, h) > max_edge:\n",
    "        scale = max_edge / float(max(w, h))\n",
    "        nw = max(patch, _round_to_multiple(int(w * scale), patch))\n",
    "        nh = max(patch, _round_to_multiple(int(h * scale), patch))\n",
    "    else:\n",
    "        nw = _round_to_multiple(w, patch)\n",
    "        nh = _round_to_multiple(h, patch)\n",
    "    return img.resize((nw, nh), Image.Resampling.LANCZOS)\n",
    "\n",
    "def enhance_doc(img: Image.Image) -> Image.Image:\n",
    "    g = ImageOps.grayscale(img)\n",
    "    g = ImageOps.autocontrast(g)\n",
    "    g = g.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "    return g.convert(\"RGB\")\n",
    "\n",
    "def is_base64_like(s: str) -> bool:\n",
    "    if not isinstance(s, str): return False\n",
    "    if s.startswith(\"data:image/\"): return True\n",
    "    return bool(re.match(r\"^[A-Za-z0-9+/=\\s]{50,}$\", s))\n",
    "\n",
    "def load_image_raw_base64(inp: str):\n",
    "    try:\n",
    "        data = inp.split(\",\", 1)[1] if inp.startswith(\"data:image/\") else inp\n",
    "        raw  = base64.b64decode(data, validate=False)\n",
    "        return Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def make_doc_views_from_base64(inp: str):\n",
    "    if not is_base64_like(inp): return None\n",
    "    img = load_image_raw_base64(inp)\n",
    "    if img is None: return None\n",
    "    base = enhance_doc(img)\n",
    "    full = _resize_to_grid(base, DOC_MAX_EDGE, DOC_PATCH)\n",
    "    return [full]\n",
    "\n",
    "# =========================\n",
    "# ⚙️ 강건한 HTTP 로더 (재시도↓, 타임아웃↓)\n",
    "# =========================\n",
    "_RETRY = Retry(\n",
    "    total=1, backoff_factor=0.4,\n",
    "    status_forcelist=(429,500,502,503,504),\n",
    "    allowed_methods=frozenset([\"GET\",\"HEAD\"])\n",
    ")\n",
    "_SESSION = requests.Session()\n",
    "_SESSION.mount(\"https://\", HTTPAdapter(max_retries=_RETRY))\n",
    "_SESSION.mount(\"http://\",  HTTPAdapter(max_retries=_RETRY))\n",
    "_DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                   \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"),\n",
    "    \"Accept\": \"image/webp,image/*;q=0.8,*/*;q=0.5\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,ko;q=0.8\",\n",
    "}\n",
    "def _origin(url: str) -> str:\n",
    "    u = urlparse(url)\n",
    "    return f\"{u.scheme}://{u.netloc}\"\n",
    "def _looks_like_image_bytes(head: bytes) -> bool:\n",
    "    return head.startswith(b\"\\xff\\xd8\") or head.startswith(b\"\\x89PNG\") or head.startswith(b\"RIFF\")\n",
    "\n",
    "def load_image_resized(inp, max_edge=MAX_IMAGE_EDGE, patch=14, timeout_s=8):\n",
    "    if isinstance(inp, str) and inp.startswith((\"http://\",\"https://\")):\n",
    "        path=_cache_path(inp, max_edge)\n",
    "        if os.path.exists(path):\n",
    "            try: return Image.open(path).convert(\"RGB\")\n",
    "            except Exception: pass\n",
    "        headers = dict(_DEFAULT_HEADERS); headers[\"Referer\"] = _origin(inp)\n",
    "        try:\n",
    "            resp = _SESSION.get(inp, headers=headers, timeout=timeout_s, allow_redirects=True, stream=True)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"[HTTP] status={resp.status_code} url={inp}\")\n",
    "                return None\n",
    "            ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            content = resp.content\n",
    "            if \"image\" not in ctype and not _looks_like_image_bytes(content[:16]):\n",
    "                print(f\"[HTTP] non-image content-type={ctype} url={inp}\")\n",
    "                return None\n",
    "            img = Image.open(io.BytesIO(content))\n",
    "            if img.mode not in (\"RGB\",\"RGBA\"):\n",
    "                img = img.convert(\"RGB\")\n",
    "            elif img.mode == \"RGBA\":\n",
    "                img = img.convert(\"RGB\")\n",
    "            if APPLY_DOC_ENHANCE_HTTP:\n",
    "                try: img = enhance_doc(img)\n",
    "                except Exception: pass\n",
    "        except Exception as e:\n",
    "            print(f\"[HTTP] fetch failed: {e} url={inp}\")\n",
    "            return None\n",
    "\n",
    "        w,h=img.size\n",
    "        if max(w,h)>max_edge:\n",
    "            scale=max_edge/float(max(w, h))\n",
    "            new_w=_round_to_multiple(int(w*scale),patch)\n",
    "            new_h=_round_to_multiple(int(h*scale),patch)\n",
    "            img=img.resize((max(patch,new_w),max(patch,new_h)), Image.Resampling.LANCZOS)\n",
    "        else:\n",
    "            img=img.resize((_round_to_multiple(w,patch),_round_to_multiple(h,patch)), Image.Resampling.LANCZOS)\n",
    "        try: img.save(path, format=\"JPEG\", quality=92)\n",
    "        except Exception: pass\n",
    "        return img\n",
    "\n",
    "    # base64\n",
    "    if isinstance(inp, str):\n",
    "        try:\n",
    "            data = inp.split(\",\", 1)[1] if inp.startswith(\"data:image/\") else inp\n",
    "            if re.match(r\"^[A-Za-z0-9+/=\\s]{100,}$\", data or \"\"):\n",
    "                raw = base64.b64decode(data, validate=False)\n",
    "                img = Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "                w, h = img.size\n",
    "                if max(w, h) > max_edge:\n",
    "                    scale = max_edge / float(max(w, h))\n",
    "                    new_w = max(patch, _round_to_multiple(int(w * scale), patch))\n",
    "                    new_h = max(patch, _round_to_multiple(int(h * scale), patch))\n",
    "                    img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "                else:\n",
    "                    img = img.resize((_round_to_multiple(w, patch), _round_to_multiple(h, patch)), Image.Resampling.LANCZOS)\n",
    "                return img\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# ✅ 태스크 라우팅(라벨 미사용, 결정 규칙만)\n",
    "# -----------------------------\n",
    "def detect_input_kind(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"unknown\"\n",
    "    s = s.strip()\n",
    "    if s.startswith((\"http://\",\"https://\")): return \"image\"\n",
    "    if re.match(r\"^data:image/\", s) or re.match(r\"^[A-Za-z0-9+/=\\s]{100,}$\", s or \"\"):\n",
    "        return \"image\"\n",
    "    return \"text\"\n",
    "\n",
    "def determine_task(input_type: str, context: str, question: str) -> str:\n",
    "    it = (input_type or \"\").strip().lower()\n",
    "    ctx = (context or \"\").strip()\n",
    "    q  = (\"\" if question is None else str(question)).strip()\n",
    "\n",
    "    if it not in (\"image\",\"text\"):\n",
    "        it = detect_input_kind(context)\n",
    "\n",
    "    if it == \"image\":\n",
    "        return \"vqa\" if q else \"captioning\"\n",
    "\n",
    "    if it == \"text\":\n",
    "        if q: return \"text_qa\"\n",
    "        if ctx.endswith(\"?\"): return \"math\" \n",
    "        return \"summarization\"\n",
    "\n",
    "    return \"summarization\"\n",
    "\n",
    "# -----------------------------\n",
    "# 입력 기반 프롬프트\n",
    "# -----------------------------\n",
    "def build_text_prompt(input_type: str, context: str, question: str):\n",
    "    itype = (input_type or \"\").strip().lower()\n",
    "    ctx = (context or \"\").strip()\n",
    "    q   = (\"\" if question is None else str(question)).strip()\n",
    "\n",
    "    task_r = determine_task(itype, ctx, q)\n",
    "\n",
    "    parts = []\n",
    "    if itype == \"text\" and ctx:\n",
    "        parts.append(ctx)\n",
    "\n",
    "    if q:\n",
    "        parts.append(\"Question: \" + q)\n",
    "    else:\n",
    "        if task_r == \"captioning\":\n",
    "            parts.append(\"Question: \" + DEFAULT_ASK_IMAGE)\n",
    "        elif task_r == \"math\":\n",
    "            parts.append(\"Question: \" + DEFAULT_ASK_MATH)\n",
    "        else:\n",
    "            parts.append(\"Question: \" + DEFAULT_ASK_TEXT)\n",
    "\n",
    "    if itype == \"image\":\n",
    "        if q:\n",
    "            parts.append(\"Constraints: Provide one concise sentence grounded only in the image.\")\n",
    "        else:\n",
    "            parts.append(\"Constraints: Provide 3–5 sentences, grounded only in the image.\")\n",
    "\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 후처리(형식 강제 + 빈 출력 가드 \"-\")\n",
    "# -----------------------------\n",
    "def _first_sentence(s: str) -> str:\n",
    "    m = re.split(r\"(?<=[.!?])\\s+\", s.strip(), maxsplit=1)\n",
    "    return m[0] if m and m[0] else s.strip()\n",
    "\n",
    "def _strip_bullets_and_chatty(s: str) -> str:\n",
    "    lines = [re.sub(r'^\\s*(?:[-*•]+|\\d+\\.)\\s*', '', ln) for ln in s.splitlines()]\n",
    "    s2 = \" \".join([ln.strip() for ln in lines if ln.strip()])\n",
    "    s2 = re.sub(r\"[👍✨💡✅❗️❗️🤔📝]+\", \"\", s2)\n",
    "    return re.sub(r\"\\s+\", \" \", s2).strip()\n",
    "\n",
    "def postprocess_output(ans: str,\n",
    "                       question: str = None,\n",
    "                       input_type: str = None,\n",
    "                       image_loaded: bool = True,\n",
    "                       context: str = None):\n",
    "    s = (ans or \"\")\n",
    "\n",
    "    for p in BANNED_PHRASES:\n",
    "        s = s.replace(p, \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    if UNKNOWN_POLICY == \"off\":\n",
    "        if re.fullmatch(r\"(?i)\\s*unknown\\s*\", s): s = \"\"\n",
    "        s = re.sub(r\"(?i)\\bunknown\\b\", \"\", s).strip()\n",
    "\n",
    "    s = re.sub(r\"(?<=\\d),(?=\\d)\", \"\", s)\n",
    "\n",
    "    it = (input_type or \"\").strip().lower()\n",
    "    q  = (question or \"\").strip()\n",
    "    ctx = (context or \"\").strip()\n",
    "    task_r = determine_task(it, ctx, q)\n",
    "\n",
    "    if task_r == \"math\" and it != \"image\":\n",
    "        if \"####\" not in s:\n",
    "            nums = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s)\n",
    "            tail = nums[-1] if nums else (s.split()[-1] if s else \"\")\n",
    "            s = f\"{s}\\n#### {tail}\".strip()\n",
    "\n",
    "    if it == \"image\":\n",
    "        s = re.sub(r\"\\s*####\\s*.*$\", \"\", s).strip()\n",
    "        if q and VQA_SHORTEN_ONE_SENTENCE:\n",
    "            s = _first_sentence(s)\n",
    "\n",
    "    if task_r == \"summarization\":\n",
    "        s = _strip_bullets_and_chatty(s)\n",
    "\n",
    "    if not s.strip():\n",
    "        s = \"-\"\n",
    "\n",
    "    return s\n",
    "\n",
    "# -----------------------------\n",
    "# ⛔ 문장 개수 상한 스톱 기준\n",
    "# -----------------------------\n",
    "class StopOnSentenceCount(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, start_pos: int, max_sentences: int):\n",
    "        self.tok = tokenizer\n",
    "        self.start_pos = int(start_pos)\n",
    "        self.max_sentences = int(max_sentences)\n",
    "        self._pat = re.compile(r'[.!?](?=\\s|$|[\"”])')\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        new_ids = input_ids[0][self.start_pos:]\n",
    "        if new_ids.numel() <= 0:\n",
    "            return False\n",
    "        text = self.tok.decode(new_ids, skip_special_tokens=True)\n",
    "        ends = self._pat.findall(text)\n",
    "        return len(ends) >= self.max_sentences\n",
    "\n",
    "# -----------------------------\n",
    "# 모델/프로세서 (QLoRA 4bit)\n",
    "# -----------------------------\n",
    "bnb = None\n",
    "if USE_4BIT:\n",
    "    try:\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes 설정 실패 → 4bit 비활성:\", e)\n",
    "        USE_4BIT = False\n",
    "        bnb = None\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb,\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, token=token)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 비전 타워 동결\n",
    "for n, p in model.named_parameters():\n",
    "    if any(k in n for k in [\"vision\",\"vision_tower\",\"clip\",\"image\",\"vision_model\"]):\n",
    "        p.requires_grad = False\n",
    "\n",
    "# LoRA\n",
    "lora = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora)\n",
    "\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False  # gradient_checkpointing 호환\n",
    "\n",
    "# trainable % 출력\n",
    "def count_trainable(m):\n",
    "    t=a=0\n",
    "    for p in m.parameters():\n",
    "        a+=p.numel()\n",
    "        if p.requires_grad: t+=p.numel()\n",
    "    return t,a\n",
    "t,a = count_trainable(model)\n",
    "print(f\"trainable params: {t:,} || all params: {a:,} || trainable%: {t/a*100:.4f}\")\n",
    "print(\"모델 + LoRA 준비 완료\")\n",
    "\n",
    "# -----------------------------\n",
    "# 데이터 로드(+클린) → 서브셋 → 90/10 분할\n",
    "# -----------------------------\n",
    "df_full = safe_read_df(CSV_TRAIN).copy()\n",
    "for col in [\"input_type\",\"input\",\"question\",\"output\",\"task\"]:\n",
    "    if col not in df_full.columns:\n",
    "        df_full[col] = \"\" if col!=\"task\" else \"unknown\"\n",
    "df_full = df_full[~df_full[\"output\"].isna()]\n",
    "df_full = df_full[df_full[\"output\"].astype(str).str.strip().str.len()>0].reset_index(drop=True)\n",
    "df_full[\"_orig_idx\"] = np.arange(len(df_full))\n",
    "\n",
    "def stratified_or_random_sample(df, n=8000, by_cols=None, seed=42):\n",
    "    n=min(n,len(df))\n",
    "    if n<=0: return df.head(0).copy()\n",
    "    if by_cols:\n",
    "        tot=len(df); parts=[]; remain=n\n",
    "        for _,g in df.groupby(by_cols, dropna=False, group_keys=False):\n",
    "            alloc=max(1,int(round(len(g)/tot*n))); take=min(alloc,len(g))\n",
    "            parts.append(g.sample(n=take, random_state=seed)); remain-=take\n",
    "        out=pd.concat(parts) if parts else df.head(0)\n",
    "        if remain>0:\n",
    "            rest=df.drop(out.index, errors=\"ignore\")\n",
    "            if len(rest)>0: out=pd.concat([out, rest.sample(n=min(remain,len(rest)), random_state=seed)])\n",
    "        if len(out)>n: out=out.sample(n=n, random_state=seed)\n",
    "        return out.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        return df.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "def stratified_train_val_split(df, val_frac=0.1, by_cols=None, seed=42):\n",
    "    df=df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    if not by_cols:\n",
    "        n_val=max(1,int(round(len(df)*val_frac)))\n",
    "        return df.iloc[n_val:].reset_index(drop=True), df.iloc[:n_val].reset_index(drop=True)\n",
    "    parts_tr,parts_val=[],[]\n",
    "    for _,g in df.groupby(by_cols, dropna=False, group_keys=False):\n",
    "        if len(g)==1: parts_tr.append(g); continue\n",
    "        n_val=max(1,int(round(len(g)*val_frac))); g=g.sample(frac=1.0, random_state=seed)\n",
    "        parts_val.append(g.iloc[:n_val]); parts_tr.append(g.iloc[n_val:])\n",
    "    tr=pd.concat(parts_tr).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    va=pd.concat(parts_val).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return tr, va\n",
    "\n",
    "if USE_TRAIN_SUBSET:\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    if os.path.exists(SUBSET_IDX_PATH):\n",
    "        try:\n",
    "            idx_df = pd.read_csv(SUBSET_IDX_PATH)\n",
    "            keep_idx = idx_df[\"_orig_idx\"].astype(int).tolist()\n",
    "            df_core = df_full.iloc[keep_idx].reset_index(drop=True)\n",
    "            print(f\"★ 부분 학습 모드(재개): subset size = {len(df_core)} (from {len(df_full)})\")\n",
    "        except Exception as e:\n",
    "            print(\"부분 인덱스 로드 실패, 새로 샘플링:\", e)\n",
    "            by_cols=[c for c in SUBSET_GROUP_CANDIDATES if c in df_full.columns]\n",
    "            df_core = stratified_or_random_sample(df_full, n=SUBSET_SIZE, by_cols=by_cols, seed=SUBSET_SEED)\n",
    "            pd.DataFrame({\"_orig_idx\": df_core[\"_orig_idx\"].tolist()}).to_csv(SUBSET_IDX_PATH, index=False)\n",
    "            print(f\"★ 부분 학습 모드(신규): subset size = {len(df_core)} → 인덱스 저장\")\n",
    "    else:\n",
    "        by_cols=[c for c in SUBSET_GROUP_CANDIDATES if c in df_full.columns]\n",
    "        df_core = stratified_or_random_sample(df_full, n=SUBSET_SIZE, by_cols=by_cols, seed=SUBSET_SEED)\n",
    "        pd.DataFrame({\"_orig_idx\": df_core[\"_orig_idx\"].tolist()}).to_csv(SUBSET_IDX_PATH, index=False)\n",
    "        print(f\"★ 부분 학습 모드(신규): subset size = {len(df_core)} → 인덱스 저장\")\n",
    "else:\n",
    "    df_core = df_full.reset_index(drop=True)\n",
    "\n",
    "by_cols_for_split=[c for c in VAL_GROUP_CANDIDATES if c in df_core.columns]\n",
    "print(\"층화 분할 기준:\", by_cols_for_split if by_cols_for_split else \"없음(랜덤)\")\n",
    "\n",
    "df_tr, df_va = stratified_train_val_split(df_core, val_frac=VAL_FRACTION, by_cols=by_cols_for_split, seed=SUBSET_SEED)\n",
    "\n",
    "pd.DataFrame({\"_orig_idx\": df_tr[\"_orig_idx\"]}).to_csv(os.path.join(SAVE_DIR, \"train_split_indices.csv\"), index=False)\n",
    "pd.DataFrame({\"_orig_idx\": df_va[\"_orig_idx\"]}).to_csv(os.path.join(SAVE_DIR, \"val_split_indices.csv\"), index=False)\n",
    "for x in (df_tr, df_va):\n",
    "    if \"_orig_idx\" in x.columns: x.drop(columns=[\"_orig_idx\"], inplace=True)\n",
    "\n",
    "print(f\"final train size: {len(df_tr)} | final val size: {len(df_va)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 데이터셋/콜레이트\n",
    "# -----------------------------\n",
    "class MultiModalDataset(TorchDataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df=df.reset_index(drop=True)\n",
    "        self.processor=processor\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row=self.df.iloc[idx]\n",
    "        input_type=row.get(\"input_type\",\"\")\n",
    "        question=row.get(\"question\",\"\")\n",
    "        context=row.get(\"input\",\"\")\n",
    "        answer=row.get(\"output\",\"\")\n",
    "\n",
    "        user_text = build_text_prompt(input_type, context, question)\n",
    "        # 학습 시에는 이미지 로딩 유지(정확도)\n",
    "        image = load_image_resized(context) if input_type==\"image\" else None\n",
    "\n",
    "        if image is not None and input_type==\"image\":\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "                {\"role\":\"user\",\"content\":[{\"type\":\"image\"},{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "            ]; images=[image]\n",
    "        else:\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "                {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "            ]; images=None\n",
    "\n",
    "        prompt=self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        enc=self.processor(text=prompt, images=images, return_tensors=\"pt\", padding=False)\n",
    "\n",
    "        # ✅ 학습 토큰 클립: 프롬프트 보존, 정답만 예산 내\n",
    "        enc_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        enc_att = enc[\"attention_mask\"].squeeze(0)\n",
    "        ans_ids = self.processor.tokenizer(\n",
    "            str(answer) if isinstance(answer,str) else \"\",\n",
    "            return_tensors=\"pt\", add_special_tokens=True, padding=False\n",
    "        ).input_ids.squeeze(0)\n",
    "        allow = max(0, MAX_TEXT_TOKENS - enc_ids.size(0))\n",
    "        if ans_ids.size(0) > allow:\n",
    "            ans_ids = ans_ids[:allow]\n",
    "\n",
    "        in_ids  = torch.cat([enc_ids, ans_ids], dim=0)\n",
    "        attn    = torch.cat([enc_att, torch.ones_like(ans_ids)], dim=0)\n",
    "\n",
    "        prompt_len = enc_ids.size(0)\n",
    "        labels = in_ids.clone()\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        item={\"input_ids\":in_ids.long(),\"attention_mask\":attn.long(),\"labels\":labels.long()}\n",
    "        if \"pixel_values\" in enc: item[\"pixel_values\"]=enc[\"pixel_values\"].squeeze(0)\n",
    "        if \"image_grid_thw\" in enc: item[\"image_grid_thw\"]=enc[\"image_grid_thw\"].squeeze(0)\n",
    "        return item\n",
    "\n",
    "def collate(batch):\n",
    "    pad_id = processor.tokenizer.pad_token_id or 0\n",
    "    def pad_1d(tensors, pad_val, dtype=torch.long):\n",
    "        m = max(t.size(0) for t in tensors); out=[]\n",
    "        for t in tensors:\n",
    "            if t.size(0)<m:\n",
    "                pad=torch.full((m-t.size(0),), pad_val, dtype=t.dtype)\n",
    "                out.append(torch.cat([t,pad], dim=0))\n",
    "            else: out.append(t)\n",
    "        return torch.stack(out, dim=0).to(dtype)\n",
    "    b={}\n",
    "    b[\"input_ids\"]      = pad_1d([x[\"input_ids\"] for x in batch], pad_id, torch.long)\n",
    "    b[\"attention_mask\"] = pad_1d([x[\"attention_mask\"] for x in batch], 0, torch.long)\n",
    "    b[\"labels\"]         = pad_1d([x[\"labels\"] for x in batch], -100, torch.long)\n",
    "    if all(\"pixel_values\" in x for x in batch):\n",
    "        b[\"pixel_values\"]=torch.stack([x[\"pixel_values\"] for x in batch], dim=0)\n",
    "    if all(\"image_grid_thw\" in x for x in batch):\n",
    "        grids=[x[\"image_grid_thw\"] for x in batch]\n",
    "        try: b[\"image_grid_thw\"]=torch.stack(grids, dim=0)\n",
    "        except Exception: b[\"image_grid_thw\"]=grids\n",
    "    return b\n",
    "\n",
    "train_ds = MultiModalDataset(df_tr, processor)\n",
    "val_ds   = MultiModalDataset(df_va, processor)\n",
    "\n",
    "# -----------------------------\n",
    "# 커스텀 지표(라우팅 기반)\n",
    "# -----------------------------\n",
    "_punc_tbl = str.maketrans(\"\", \"\", string.punctuation)\n",
    "def _normalize(s):\n",
    "    s = (\"\" if s is None else str(s)).strip().lower()\n",
    "    s = s.translate(_punc_tbl)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def exact_match(pred, ref): return 1.0 if _normalize(pred) == _normalize(ref) else 0.0\n",
    "\n",
    "def token_f1(pred, ref):\n",
    "    p = _normalize(pred).split(); r = _normalize(ref).split()\n",
    "    if len(p)==0 and len(r)==0: return 1.0\n",
    "    if len(p)==0 or len(r)==0:  return 0.0\n",
    "    from collections import Counter\n",
    "    pc, rc = Counter(p), Counter(r)\n",
    "    common = sum((pc & rc).values())\n",
    "    if common==0: return 0.0\n",
    "    prec = common / max(1, len(p)); rec  = common / max(1, len(r))\n",
    "    return (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "\n",
    "def lcs_len(a, b):\n",
    "    na, nb = len(a), len(b)\n",
    "    dp = [[0]*(nb+1) for _ in range(na+1)]\n",
    "    for i in range(1, na+1):\n",
    "        ai=a[i-1]; row=dp[i]; prow=dp[i-1]\n",
    "        for j in range(1, nb+1):\n",
    "            row[j]=prow[j-1]+1 if ai==b[j-1] else max(prow[j], row[j-1])\n",
    "    return dp[na][nb]\n",
    "\n",
    "def rouge_l_f1(pred, ref):\n",
    "    p = _normalize(pred).split(); r = _normalize(ref).split()\n",
    "    if len(p)==0 and len(r)==0: return 1.0\n",
    "    if len(p)==0 or len(r)==0:  return 0.0\n",
    "    l = lcs_len(p, r); prec = l / max(1, len(p)); rec  = l / max(1, len(r))\n",
    "    return (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "\n",
    "def numeric_em(pred, ref, tol=1e-3):\n",
    "    def _num(s):\n",
    "        m = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s.replace(\",\", \"\"))\n",
    "        return m[0] if m else None\n",
    "    pn, rn = _num(pred), _num(ref)\n",
    "    if pn is None or rn is None: return exact_match(pred, ref)\n",
    "    try:\n",
    "        pn, rn = float(pn), float(rn)\n",
    "        if rn==0: return 1.0 if abs(pn-rn)<=tol else 0.0\n",
    "        return 1.0 if abs(pn-rn)/max(1e-12, abs(rn)) <= 1e-3 or abs(pn-rn)<=tol else 0.0\n",
    "    except: return exact_match(pred, ref)\n",
    "\n",
    "def score_sample_routed(task_r, pred, ref):\n",
    "    t = (task_r or \"\").strip().lower()\n",
    "    if t in [\"text_qa\", \"vqa\"]:\n",
    "        return {\"em\": exact_match(pred, ref), \"f1\": token_f1(pred, ref)}\n",
    "    elif t in [\"summarization\", \"captioning\"]:\n",
    "        return {\"rougeL\": rouge_l_f1(pred, ref)}\n",
    "    elif t in [\"math\"]:\n",
    "        return {\"em\": numeric_em(pred, ref)}\n",
    "    else:\n",
    "        return {\"f1\": token_f1(pred, ref)}\n",
    "\n",
    "def collapse_task_metric(task_r, d):\n",
    "    t = (task_r or \"\").strip().lower()\n",
    "    if t in [\"text_qa\", \"vqa\"]:\n",
    "        return d.get(\"f1\", d.get(\"em\", 0.0))\n",
    "    elif t in [\"summarization\", \"captioning\"]:\n",
    "        return d.get(\"rougeL\", 0.0)\n",
    "    elif t in [\"math\"]:\n",
    "        return d.get(\"em\", 0.0)\n",
    "    else:\n",
    "        return d.get(\"f1\", 0.0)\n",
    "\n",
    "# -----------------------------\n",
    "# 평가 서브셋 구성(태스크별 균형 샘플링)\n",
    "# -----------------------------\n",
    "def build_eval_subset(df_va, per_task=EVAL_SAMPLES_PER_TASK, max_total=EVAL_MAX_TOTAL_SAMPLES, seed=SUBSET_SEED):\n",
    "    if len(df_va)==0: return df_va\n",
    "    tmp = df_va.copy()\n",
    "    tmp[\"_routed_task\"] = tmp.apply(lambda r: determine_task(r.get(\"input_type\",\"\"), r.get(\"input\",\"\"), r.get(\"question\",\"\")), axis=1)\n",
    "    parts=[]\n",
    "    rng = np.random.RandomState(seed)\n",
    "    for t, g in tmp.groupby(\"_routed_task\", dropna=False):\n",
    "        take = min(per_task, len(g))\n",
    "        parts.append(g.sample(n=take, random_state=rng))\n",
    "    out = pd.concat(parts).sample(frac=1.0, random_state=rng).reset_index(drop=True)\n",
    "    if len(out) > max_total:\n",
    "        out = out.sample(n=max_total, random_state=rng).reset_index(drop=True)\n",
    "    return out.drop(columns=[\"_routed_task\"], errors=\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# 시간 제한 + 주기 저장\n",
    "# -----------------------------\n",
    "class TimeLimitCallback(TrainerCallback):\n",
    "    def __init__(self, max_seconds, autosave_every_sec=AUTOSAVE_EVERY_MIN*60):\n",
    "        self.start = time.time()\n",
    "        self.max_seconds = max_seconds\n",
    "        self.autosave_every_sec = autosave_every_sec\n",
    "        self.last_save = self.start\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        now=time.time()\n",
    "        if now - self.last_save >= self.autosave_every_sec:\n",
    "            control.should_save = True\n",
    "            self.last_save = now\n",
    "        if now - self.start >= self.max_seconds:\n",
    "            print(\"\\n⏰ 시간 제한 도달: 체크포인트 저장 후 학습 종료합니다.\")\n",
    "            control.should_save = True\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "# -----------------------------\n",
    "# 단순 생성(평가/추론용, dedup/샘플링 없음)\n",
    "# -----------------------------\n",
    "def _generate_once_greedy(model, enc, max_new_tokens: int, stopping_criteria=None, eos_token_id=None, pad_token_id=None):\n",
    "    return model.generate(\n",
    "        **enc,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=pad_token_id if pad_token_id is not None else eos_token_id\n",
    "    )\n",
    "\n",
    "def _decode_new_tokens(processor, gen_ids, enc):\n",
    "    new_ids = gen_ids[0][enc[\"input_ids\"].shape[-1]:]\n",
    "    return processor.tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def _fast_generate_row(model, processor, row, stage_name: str, *_args_ignored):\n",
    "    input_type=row.get(\"input_type\",\"\"); question=row.get(\"question\",\"\"); context=row.get(\"input\",\"\")\n",
    "    user_text = build_text_prompt(input_type, context, question)\n",
    "    routed = determine_task(input_type, context, question)\n",
    "\n",
    "    # 평가/추론에서 이미지 스킵 옵션 반영\n",
    "    use_images = False\n",
    "    images = None\n",
    "    if input_type==\"image\":\n",
    "        if stage_name==\"eval\" and EVAL_SKIP_IMAGE_DOWNLOAD:\n",
    "            use_images = False\n",
    "        elif stage_name==\"test\" and INFER_SKIP_IMAGE_DOWNLOAD:\n",
    "            use_images = False\n",
    "        else:\n",
    "            img = make_doc_views_from_base64(context) or [load_image_resized(context)]\n",
    "            img = [x for x in img if x is not None]\n",
    "            if len(img)>0:\n",
    "                images = img\n",
    "                use_images = True\n",
    "\n",
    "    if use_images:\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[*([{\"type\":\"image\"}]*len(images)), {\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]\n",
    "    else:\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    enc = processor(text=prompt, images=(images if use_images else None), return_tensors=\"pt\", padding=False)\n",
    "    if torch.cuda.is_available(): enc = {k:v.to(model.device) for k,v in enc.items()}\n",
    "\n",
    "    # per-task 토큰/스톱\n",
    "    start_pos = enc[\"input_ids\"].shape[-1]\n",
    "    sc = StoppingCriteriaList()\n",
    "    if stage_name == \"eval\":\n",
    "        if routed == \"captioning\":\n",
    "            local_max = CAPTION_MAX_NEW_TOKENS_EVAL\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, CAPTION_MAX_SENTENCES))\n",
    "        elif routed == \"summarization\":\n",
    "            local_max = SUM_MAX_NEW_TOKENS_EVAL\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, SUM_MAX_SENTENCES))\n",
    "        elif routed == \"math\":\n",
    "            local_max = MATH_MAX_NEW_TOKENS_EVAL\n",
    "        else:\n",
    "            local_max = MAX_NEW_TOKENS_EVAL\n",
    "    else:  # test\n",
    "        if routed == \"captioning\":\n",
    "            local_max = CAPTION_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, CAPTION_MAX_SENTENCES))\n",
    "        elif routed == \"summarization\":\n",
    "            local_max = SUM_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, SUM_MAX_SENTENCES))\n",
    "        elif routed == \"math\":\n",
    "            local_max = MATH_MAX_NEW_TOKENS_TEST\n",
    "        else:\n",
    "            local_max = MAX_NEW_TOKENS_TEST\n",
    "\n",
    "    gen_ids = _generate_once_greedy(\n",
    "        model, enc, local_max,\n",
    "        stopping_criteria=sc,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    pred = _decode_new_tokens(processor, gen_ids, enc)\n",
    "    return postprocess_output(pred, question=question, input_type=input_type, image_loaded=use_images, context=context)\n",
    "\n",
    "# -----------------------------\n",
    "# 콜백: 간이 평가(라우팅 기준) + ES (빠른 모드)\n",
    "# -----------------------------\n",
    "class MetricEvalEarlyStopCallback(TrainerCallback):\n",
    "    def __init__(self, processor, eval_df, final_dir,\n",
    "                 every_steps=EVAL_EVERY_STEPS, patience=ES_PATIENCE, min_delta=ES_MIN_DELTA):\n",
    "        self.processor = processor\n",
    "        self.eval_df   = eval_df\n",
    "        self.final_dir = final_dir\n",
    "        self.every_steps = every_steps\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.bad_cnt = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _evaluate_small(self, model):\n",
    "        if len(self.eval_df)==0: return 0.0, {}\n",
    "        model.eval()\n",
    "        per_task_scores = {}\n",
    "\n",
    "        for _, row in self.eval_df.iterrows():\n",
    "            ref  = str(row.get(\"output\",\"\"))\n",
    "            tdet = determine_task(row.get(\"input_type\",\"\"), row.get(\"input\",\"\"), row.get(\"question\",\"\"))\n",
    "            try:\n",
    "                pred = _fast_generate_row(model, self.processor, row, \"eval\")\n",
    "            except Exception:\n",
    "                pred = \"-\"\n",
    "            sdict = score_sample_routed(tdet, pred, ref)\n",
    "            s     = collapse_task_metric(tdet, sdict)\n",
    "            per_task_scores.setdefault(tdet, []).append(float(s))\n",
    "\n",
    "        task_avgs = {t: (sum(v)/len(v) if len(v)>0 else 0.0) for t,v in per_task_scores.items()}\n",
    "        macro = sum(task_avgs.values())/max(1, len(task_avgs))\n",
    "        return macro, task_avgs\n",
    "\n",
    "    def _maybe_eval_and_es(self, state, control, model, tag):\n",
    "        if len(self.eval_df)==0: return control\n",
    "        macro, task_avgs = self._evaluate_small(model)\n",
    "        msg = f\"[{tag}] step={state.global_step} macro={macro:.4f} | \" + \" \".join([f\"{k}:{v:.3f}\" for k,v in sorted(task_avgs.items())])\n",
    "        print(msg)\n",
    "        try:\n",
    "            os.makedirs(self.final_dir, exist_ok=True)\n",
    "            with open(os.path.join(self.final_dir, \"last_eval_metrics.json\"), \"w\") as f:\n",
    "                json.dump({\"step\": int(state.global_step), \"macro\": float(macro), \"by_task\": {k: float(v) for k,v in task_avgs.items()}}, f)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        improved = (self.best is None) or (macro > self.best + self.min_delta)\n",
    "        if improved:\n",
    "            self.best = macro; self.bad_cnt = 0\n",
    "            try:\n",
    "                os.makedirs(self.final_dir, exist_ok=True)\n",
    "                model.save_pretrained(self.final_dir); self.processor.save_pretrained(self.final_dir)\n",
    "                print(f\"💾 새 베스트 저장 (macro={macro:.4f}) → {self.final_dir}\")\n",
    "            except Exception as e:\n",
    "                print(\"베스트 저장 실패:\", e)\n",
    "            control.should_save = True\n",
    "        else:\n",
    "            self.bad_cnt += 1\n",
    "            print(f\"↳ 개선 없음 ({self.bad_cnt}/{self.patience})\")\n",
    "            if self.bad_cnt >= self.patience:\n",
    "                print(\"🛑 Early Stop 발동 (patience 소진)\")\n",
    "                control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        if model is None: return control\n",
    "        if state.global_step>0 and (state.global_step % self.every_steps == 0):\n",
    "            control = self._maybe_eval_and_es(state, control, model, tag=\"interval\")\n",
    "        return control\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        control = self._maybe_eval_and_es(state, control, model, tag=\"epoch_end\")\n",
    "        return control\n",
    "\n",
    "# -----------------------------\n",
    "# 학습 파라미터 & 트레이너\n",
    "# -----------------------------\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=CKPT_DIR,\n",
    "    per_device_train_batch_size=PER_DEV_TRAIN_BS,\n",
    "    per_device_eval_batch_size=PER_DEV_EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=USE_BF16,\n",
    "    fp16=USE_FP16,\n",
    "    logging_steps=20,\n",
    "    save_steps=400,                # 평가 주기와 보조\n",
    "    save_total_limit=4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    save_safetensors=True,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "# 평가 서브셋 구축 (속도↑)\n",
    "eval_subset_df = build_eval_subset(df_va, per_task=EVAL_SAMPLES_PER_TASK, max_total=EVAL_MAX_TOTAL_SAMPLES, seed=SUBSET_SEED)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,  # (HF 트레이너용, 실제 평가는 커스텀 콜백에서 eval_subset_df 사용)\n",
    "    data_collator=collate,\n",
    "    callbacks=[\n",
    "        TimeLimitCallback(max_seconds=int(LIMIT_HOURS*3600)),\n",
    "        MetricEvalEarlyStopCallback(\n",
    "            processor=processor,\n",
    "            eval_df=eval_subset_df,\n",
    "            final_dir=FINAL_DIR,\n",
    "            every_steps=EVAL_EVERY_STEPS,\n",
    "            patience=ES_PATIENCE,\n",
    "            min_delta=ES_MIN_DELTA,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Torch<2.6 CVE 우회: weights-only resume\n",
    "# -----------------------------\n",
    "def try_load_adapter_weights_from(path):\n",
    "    st_path = os.path.join(path, \"adapter_model.safetensors\")\n",
    "    if not os.path.exists(st_path):\n",
    "        return False\n",
    "    try:\n",
    "        sd = safe_load_file(st_path)\n",
    "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "        print(f\"weights-only resume: missing={len(missing)}, unexpected={len(unexpected)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"weights-only load 실패:\", e)\n",
    "        return False\n",
    "\n",
    "def torch_version_tuple():\n",
    "    ver = torch.__version__.split('+')[0]\n",
    "    return tuple(int(x) for x in ver.split('.')[:3])\n",
    "\n",
    "last_ckpt = get_last_checkpoint(CKPT_DIR) if os.path.isdir(CKPT_DIR) else None\n",
    "can_secure_resume = torch_version_tuple() >= (2,6,0)\n",
    "\n",
    "if last_ckpt and not can_secure_resume:\n",
    "    print(f\"🔁 Torch<{2.6}: optimizer 복원 생략, 가중치만 이어서 학습 ({last_ckpt})\")\n",
    "    ok = try_load_adapter_weights_from(last_ckpt)\n",
    "    if not ok and os.path.isdir(FINAL_DIR):\n",
    "        print(\"체크포인트 로드 실패 → FINAL_DIR에서 가중치 복원 시도\")\n",
    "        ok = try_load_adapter_weights_from(FINAL_DIR)\n",
    "    last_ckpt = None\n",
    "elif last_ckpt:\n",
    "    print(f\"🔁 체크포인트에서 재개: {last_ckpt}\")\n",
    "else:\n",
    "    print(\"🚀 새 학습 시작\")\n",
    "\n",
    "# -----------------------------\n",
    "# 학습 & 저장\n",
    "# -----------------------------\n",
    "try:\n",
    "    if RUN_TRAIN:\n",
    "        trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "    else:\n",
    "        print(\"🚫 RUN_TRAIN=False: 학습을 건너뜁니다. (바로 추론 단계로)\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 학습 중 예외 발생: {e}\\n현재 상태 저장을 시도합니다.\")\n",
    "finally:\n",
    "    os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "    try:\n",
    "        model.save_pretrained(FINAL_DIR)\n",
    "        processor.save_pretrained(FINAL_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"최종 저장 중 경고:\", e)\n",
    "    print(\"모델/프로세서 저장 완료 →\", FINAL_DIR)\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 테스트셋 추론 → submission.csv (부분 저장/재개)\n",
    "# -----------------------------\n",
    "print(\"Generating submission.csv from test set...\")\n",
    "model.eval(); torch.set_grad_enabled(False)\n",
    "df_test = safe_read_df(CSV_TEST).reset_index(drop=True)\n",
    "\n",
    "def build_messages_for_infer(row):\n",
    "    input_type = row.get(\"input_type\",\"\")\n",
    "    question   = row.get(\"question\",\"\")\n",
    "    context    = row.get(\"input\",\"\")\n",
    "    user_text  = build_text_prompt(input_type, context, question)\n",
    "    routed     = determine_task(input_type, context, question)\n",
    "\n",
    "    use_images = False\n",
    "    images = None\n",
    "    if input_type==\"image\":\n",
    "        if INFER_SKIP_IMAGE_DOWNLOAD:\n",
    "            use_images = False\n",
    "        else:\n",
    "            img = make_doc_views_from_base64(context) or [load_image_resized(context)]\n",
    "            img = [x for x in img if x is not None]\n",
    "            if len(img)>0:\n",
    "                images = img\n",
    "                use_images = True\n",
    "\n",
    "    if use_images:\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[*([{\"type\":\"image\"}]*len(images)), {\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]; return messages, images\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]; return messages, None\n",
    "\n",
    "# 부분 저장 이어받기\n",
    "processed={}\n",
    "if os.path.exists(SUBMIT_PART):\n",
    "    try:\n",
    "        dfp=pd.read_csv(SUBMIT_PART)\n",
    "        for _, r in dfp.iterrows(): processed[int(r[\"id\"])]=str(r[\"output\"])\n",
    "        print(f\"🔁 기존 부분 결과 로드: {len(processed)}개\")\n",
    "    except Exception as e:\n",
    "        print(\"부분 결과 로드 실패, 새로 시작:\", e)\n",
    "\n",
    "preds_map=dict(processed)\n",
    "elapsed_total=time.time()-GLOBAL_START\n",
    "limit_sec=max(0,int(LIMIT_HOURS*3600)-int(elapsed_total))\n",
    "infer_start=time.time()\n",
    "def time_left_ok(): return (time.time()-infer_start) < max(0, limit_sec-5*60)\n",
    "\n",
    "todo_ids=[i for i in range(len(df_test)) if i not in preds_map]\n",
    "for idx, i in enumerate(todo_ids, 1):\n",
    "    if not time_left_ok():\n",
    "        print(\"⏰ 추론 시간 한계 도달. 부분 결과 저장 후 종료.\"); break\n",
    "    row=df_test.iloc[i]\n",
    "    try:\n",
    "        messages, images = build_messages_for_infer(row)\n",
    "        prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        enc = processor(text=prompt, images=images, return_tensors=\"pt\", padding=False)\n",
    "        if torch.cuda.is_available(): enc = {k:v.to(model.device) for k,v in enc.items()}\n",
    "\n",
    "        tdet = determine_task(row.get(\"input_type\",\"\"), row.get(\"input\",\"\"), row.get(\"question\",\"\"))\n",
    "\n",
    "        # per-task 토큰/스톱\n",
    "        start_pos = enc[\"input_ids\"].shape[-1]\n",
    "        sc = StoppingCriteriaList()\n",
    "        if tdet == \"captioning\":\n",
    "            local_max = CAPTION_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, CAPTION_MAX_SENTENCES))\n",
    "        elif tdet == \"summarization\":\n",
    "            local_max = SUM_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, SUM_MAX_SENTENCES))\n",
    "        elif tdet == \"math\":\n",
    "            local_max = MATH_MAX_NEW_TOKENS_TEST\n",
    "        else:\n",
    "            local_max = MAX_NEW_TOKENS_TEST\n",
    "\n",
    "        # 추론도 greedy + 스톱 기준\n",
    "        gen_ids = _generate_once_greedy(\n",
    "            model, enc, local_max,\n",
    "            stopping_criteria=sc,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        pred = _decode_new_tokens(processor, gen_ids, enc)\n",
    "        pred = postprocess_output(pred,\n",
    "                                  question=row.get(\"question\",\"\"),\n",
    "                                  input_type=row.get(\"input_type\",\"\"),\n",
    "                                  image_loaded=(images is not None),\n",
    "                                  context=row.get(\"input\",\"\"))\n",
    "        if not pred.strip():\n",
    "            pred = \"-\"\n",
    "    except Exception:\n",
    "        pred = \"-\"\n",
    "\n",
    "    preds_map[i] = pred\n",
    "\n",
    "    if (idx % INFER_SAVE_EVERY == 0) or (idx == len(todo_ids)):\n",
    "        tmp = pd.DataFrame({\"id\": sorted(preds_map.keys()),\n",
    "                            \"output\": [preds_map[k] for k in sorted(preds_map.keys())]})\n",
    "        tmp[\"output\"] = tmp[\"output\"].apply(lambda x: x if str(x).strip() else \"-\")\n",
    "        tmp.to_csv(SUBMIT_PART, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[{idx}/{len(todo_ids)}] 부분 저장 → {SUBMIT_PART}\")\n",
    "\n",
    "all_ids = list(range(len(df_test)))\n",
    "outputs = [preds_map.get(i, \"-\") for i in all_ids]\n",
    "outputs = [o if str(o).strip() else \"-\" for o in outputs]\n",
    "pd.DataFrame({\"id\": all_ids, \"output\": outputs}).to_csv(SUBMIT_OUT, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ submission.csv saved to:\", SUBMIT_OUT)\n",
    "print(\"✅ 부분 저장본(백업):\", SUBMIT_PART)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPAEWbeFYsyaU07sXzFLgUu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
