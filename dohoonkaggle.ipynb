{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_xp_L-tu4P4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ï∂©Îèå Î∞©ÏßÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pme5LIJCu_Nw"
   },
   "outputs": [],
   "source": [
    "!pip -q uninstall -y torchaudio\n",
    "\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install \"torch==2.3.1+cu121\" \"torchvision==0.18.1+cu121\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip -q install \"transformers>=4.43.3\" \"accelerate>=0.33.0\" \"peft>=0.12.0\" \"datasets\" \"einops\" \"pillow\"\n",
    "!pip -q install \"bitsandbytes==0.43.3\"\n",
    "!pip -q install qwen-vl-utils\n",
    "!pip -q install --upgrade \"transformers>=4.45.2\" \"accelerate>=0.34.2\" \"qwen-vl-utils>=0.0.8\"\n",
    "!pip install -q triton\n",
    "!pip install -q bitsandbytes\n",
    "!pip -q install \"triton==2.2.0\"\n",
    "!pip -q install -U transformers accelerate peft pillow einops\n",
    "!pip -q install -U bitsandbytes\n",
    "\n",
    "!pip -q uninstall -y torch torchvision torchaudio triton xformers \\\n",
    "  nvidia-cublas-cu12 nvidia-cudnn-cu12 nvidia-cuda-nvrtc-cu12 \\\n",
    "  nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cusparse-cu12\n",
    "\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "\n",
    "import torch\n",
    "print(\"torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
    "\n",
    "!pip -q install -U \"transformers>=4.46.0\" \"accelerate>=0.33.0\" \"peft>=0.11.1\" pillow einops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Î©ÄÌã∞Î™®Îã¨ ÌïôÏäµ Î∞è Ï∂îÎ°† ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, time, random, math, gc, re, string, hashlib, warnings, logging, json, base64\n",
    "import torch, pandas as pd, numpy as np\n",
    "from PIL import Image, ImageFile, ImageOps, ImageFilter\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from urllib.parse import urlparse\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from safetensors.torch import load_file as safe_load_file\n",
    "\n",
    "# ---- (ÏΩòÏÜî Î¨µÏùå Ïú†ÏßÄ) ----\n",
    "os.environ.setdefault(\"TRANSFORMERS_VERBOSITY\", \"error\")\n",
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.checkpoint\")\n",
    "Image.MAX_IMAGE_PIXELS = 300_000_000\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "GLOBAL_START = time.time()\n",
    "\n",
    "# -----------------------------\n",
    "# üß∑ ÏÇ¨Ïö©Ïûê Ï°∞Ï†ï ÏßÄÏ†ê\n",
    "# -----------------------------\n",
    "MODEL_ID    = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "CSV_TRAIN   = \"/content/drive/MyDrive/deeplearningchallenge/deep_chal_multitask_dataset.parquet\"\n",
    "CSV_TEST    = \"/content/drive/MyDrive/deeplearningchallenge/deep_chal_multitask_dataset_test.parquet\"\n",
    "\n",
    "SAVE_DIR    = \"/content/drive/MyDrive/aju_ver17/outputs_lora_total\"\n",
    "CKPT_DIR    = os.path.join(SAVE_DIR, \"checkpoints\")\n",
    "FINAL_DIR   = SAVE_DIR\n",
    "SUBMIT_OUT  = \"/content/drive/MyDrive/aju_ver17/submission.csv\"\n",
    "SUBMIT_PART = SUBMIT_OUT + \".partial\"\n",
    "IMG_CACHE_DIR   = os.path.join(SAVE_DIR, \"img_cache\")\n",
    "SUBSET_IDX_PATH = os.path.join(SAVE_DIR, \"train_subset_indices.csv\")\n",
    "\n",
    "# Toggle: ÌïôÏäµ/Ï∂îÎ°†\n",
    "RUN_TRAIN = True\n",
    "\n",
    "# Î∂ÄÎ∂Ñ ÌïôÏäµ ÏÑúÎ∏åÏÖã\n",
    "USE_TRAIN_SUBSET = True\n",
    "SUBSET_SIZE      = 58000\n",
    "SUBSET_SEED      = 42\n",
    "SUBSET_GROUP_CANDIDATES = [\"input_type\", \"task\", \"lang\"]\n",
    "\n",
    "# ÎÇ¥Î∂Ä Î∂ÑÌï†\n",
    "VAL_FRACTION   = 0.10\n",
    "VAL_GROUP_CANDIDATES = [\"type\", \"task\", \"category\", \"label\", \"input_type\", \"lang\"]\n",
    "\n",
    "# ÏãúÍ∞Ñ/Ï†ÄÏû•\n",
    "LIMIT_HOURS          = 23\n",
    "AUTOSAVE_EVERY_MIN   = 30\n",
    "INFER_SAVE_EVERY     = 100\n",
    "\n",
    "# Î©îÎ™®Î¶¨/ÏÜçÎèÑ\n",
    "USE_4BIT        = True\n",
    "MAX_TEXT_TOKENS = 1024\n",
    "\n",
    "# üîÅ Ïù¥ÎØ∏ÏßÄ Îã®Ïùº Ïä§Ìéô\n",
    "MAX_IMAGE_EDGE  = 896\n",
    "DOC_MAX_EDGE    = 896\n",
    "DOC_PATCH       = 14\n",
    "APPLY_DOC_ENHANCE_HTTP = True\n",
    "\n",
    "LORA_R          = 32\n",
    "GRAD_ACC_STEPS  = 16\n",
    "PER_DEV_TRAIN_BS= 1\n",
    "PER_DEV_EVAL_BS = 1\n",
    "MAX_GRAD_NORM   = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# ‚ö°Ô∏èÏÜçÎèÑ/Ï§ëÎ≥µ Í¥ÄÎ†® ÌÜ†Í∏Ä\n",
    "# -----------------------------\n",
    "EVAL_EVERY_STEPS        = 400   # ‚Üê 400ÏúºÎ°ú Í≥†Ï†ï\n",
    "ES_PATIENCE             = 2     # ‚Üê Îçî Îπ†Î•∏ ÏñºÎ¶¨Ïä§ÌÉë\n",
    "ES_MIN_DELTA            = 1e-6\n",
    "EVAL_SAMPLES_PER_TASK   = 40    # ÌÉúÏä§ÌÅ¨Î≥Ñ ÏÉòÌîå Ïàò\n",
    "EVAL_MAX_TOTAL_SAMPLES  = 200   # Ï†ÑÏ≤¥ ÏÉÅÌïú\n",
    "MAX_NEW_TOKENS_EVAL     = 96    # ÌèâÍ∞Ä Í∏∞Î≥∏\n",
    "MAX_NEW_TOKENS_TEST     = 128   # Ï∂îÎ°† Í∏∞Î≥∏\n",
    "\n",
    "# üîé (Ïù¥Ï†Ñ Ìò∏Ìôò) ÏöîÏïΩ Ï†ÑÏö© ÎîîÏΩîÎî© ÏÉÅÌïú(ÎØ∏ÏÇ¨Ïö© Í∞ÄÎä•)\n",
    "MAX_NEW_TOKENS_EVAL_SUM = 144\n",
    "MAX_NEW_TOKENS_TEST_SUM = 196\n",
    "\n",
    "NO_REPEAT_NGRAM_SIZE    = 4\n",
    "REPETITION_PENALTY      = 1.05\n",
    "\n",
    "# ================== Í∑úÏπô ÌÜ†Í∏Ä ==================\n",
    "UNKNOWN_POLICY        = \"off\"     # Unknown ÏôÑÏ†Ñ Ï∞®Îã®\n",
    "CITING_ENFORCEMENT    = \"off\"\n",
    "ANTI_REPEAT_POLICY    = \"off\"     # ‚Üê ÌèâÍ∞Ä Ï§ë Ïû¨ÏÉùÏÑ± Î£®ÌîÑ Ìï¥Ï†ú\n",
    "VQA_SHORTEN_ONE_SENTENCE = True\n",
    "\n",
    "# ÌèâÍ∞Ä/Ï∂îÎ°† Ï†ÑÏö© Îπ†Î•∏ Î™®Îìú\n",
    "EVAL_GREEDY_ONLY           = True\n",
    "EVAL_SKIP_IMAGE_DOWNLOAD   = True   # ÌèâÍ∞Ä Ïãú Ïù¥ÎØ∏ÏßÄ Î°úÎî© Ïä§ÌÇµ(ÏÜçÎèÑ‚Üë)\n",
    "INFER_GREEDY_ONLY          = True\n",
    "INFER_SKIP_IMAGE_DOWNLOAD  = False  # Ï†úÏ∂ú Ï†ïÌôïÎèÑ ÏúÑÌï¥ Í∏∞Î≥∏ False Ïú†ÏßÄ\n",
    "\n",
    "# =================================================\n",
    "# ‚úÖ Îã®Ïùº ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏\n",
    "SYSTEM_INST = (\n",
    "\"\"\"You are a multimodal assistant. Follow the instructions precisely for each task type based on the input.\n",
    "\n",
    "### Task Definitions & Output Formats ###\n",
    "# 1. Image Captioning: Image is provided, but no question. -> Output: 3‚Äì5 sentences (not fewer than 3, not more than 5).\n",
    "# 2. Visual Question Answering (VQA): Both an image and a question are provided. -> Output: one concise sentence.\n",
    "# 3. Math Problem: Only text is provided, the 'question' field is empty, and the text ends with a '?'. -> Output: 1‚Äì3 short lines of reasoning, ending with \"#### {answer}\".\n",
    "# 4. Text Summarization: Only text is provided, no question, and it does not end with a '?'. -> Output: a coherent paragraph, up to 6 sentences (avoid bullet points and chatty tone).\n",
    "# 5. Text Question Answering (Text QA): Both text and a question are provided. -> Output: a direct answer or a JSON string if explicitly requested.\n",
    "\n",
    "Formatting: plain text unless strictly-JSON is explicitly requested. Do not include external knowledge beyond the given inputs.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "DEFAULT_ASK_IMAGE = \"Describe the image in detail, in 3 to 5 sentences.\"\n",
    "DEFAULT_ASK_TEXT  = \"Summarize the key points of the text in a concise paragraph (up to 6 sentences).\"\n",
    "DEFAULT_ASK_MATH  = \"Solve the math problem, showing 1‚Äì4 compact steps, with the final answer prefixed by '#### ' on the last line.\"\n",
    "\n",
    "# üîí Í∏àÏπôÏñ¥(ÎîîÏΩîÎî© ÌõÑ Ï†úÍ±∞)\n",
    "BANNED_PHRASES = [\n",
    "    \"I'm sorry\", \"I am sorry\", \"I apologize\", \"Apologies\",\n",
    "    \"As an AI\", \"as a language model\",\n",
    "    \"I don't have\", \"I do not have\",\n",
    "    \"I cannot\", \"I can't\", \"cannot provide\", \"can't provide\",\n",
    "    \"I have no information\", \"no information about\",\n",
    "    \"I cannot answer\", \"I can't answer\", \"Hello! How can I assist you today?\"\n",
    "]\n",
    "\n",
    "# ÌÜ†ÌÅ∞ (ÏÑ†ÌÉù)\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token, add_to_git_credential=True)\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = token\n",
    "\n",
    "# Ïû¨ÌòÑÏÑ±\n",
    "torch.manual_seed(SUBSET_SEED); random.seed(SUBSET_SEED); np.random.seed(SUBSET_SEED)\n",
    "print(\"torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n",
    "\n",
    "# ÏÑ±Îä•/ÏïàÏ†ïÌôî\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "USE_FP16 = not USE_BF16 and torch.cuda.is_available()\n",
    "\n",
    "# -----------------------------\n",
    "# üî¢ Per-task ÌÜ†ÌÅ∞ ÏòàÏÇ∞ & Î¨∏Ïû• ÏÉÅÌïú\n",
    "# -----------------------------\n",
    "CAPTION_MAX_NEW_TOKENS_EVAL = MAX_NEW_TOKENS_EVAL * 2\n",
    "CAPTION_MAX_NEW_TOKENS_TEST = MAX_NEW_TOKENS_TEST * 2\n",
    "CAPTION_MAX_SENTENCES = 5\n",
    "\n",
    "MATH_MAX_NEW_TOKENS_EVAL = MAX_NEW_TOKENS_EVAL * 2\n",
    "MATH_MAX_NEW_TOKENS_TEST = MAX_NEW_TOKENS_TEST * 2\n",
    "\n",
    "SUM_MAX_NEW_TOKENS_EVAL = MAX_NEW_TOKENS_EVAL * 4\n",
    "SUM_MAX_NEW_TOKENS_TEST = MAX_NEW_TOKENS_TEST * 4\n",
    "SUM_MAX_SENTENCES = 6\n",
    "\n",
    "# -----------------------------\n",
    "# Ïú†Ìã∏\n",
    "# -----------------------------\n",
    "def safe_read_df(path):\n",
    "    try:\n",
    "        if path.endswith(\".parquet\"): return pd.read_parquet(path)\n",
    "        if path.endswith(\".csv\"):     return pd.read_csv(path)\n",
    "        try:                          return pd.read_parquet(path)\n",
    "        except Exception:             return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ÌååÏùº ÏùΩÍ∏∞ Ïã§Ìå®: {path} | {e}\")\n",
    "\n",
    "def _round_to_multiple(x, m): return max(m, int(round(x/m))*m)\n",
    "\n",
    "def _cache_path(url: str, max_edge: int):\n",
    "    os.makedirs(IMG_CACHE_DIR, exist_ok=True)\n",
    "    h = hashlib.sha1(f\"{url}|{max_edge}\".encode()).hexdigest()\n",
    "    return os.path.join(IMG_CACHE_DIR, f\"{h}.jpg\")\n",
    "\n",
    "# ===== Î¨∏ÏÑú Ïù¥ÎØ∏ÏßÄ Î≥¥Ï†ï =====\n",
    "def _resize_to_grid(img: Image.Image, max_edge=DOC_MAX_EDGE, patch=DOC_PATCH):\n",
    "    w, h = img.size\n",
    "    if max(w, h) > max_edge:\n",
    "        scale = max_edge / float(max(w, h))\n",
    "        nw = max(patch, _round_to_multiple(int(w * scale), patch))\n",
    "        nh = max(patch, _round_to_multiple(int(h * scale), patch))\n",
    "    else:\n",
    "        nw = _round_to_multiple(w, patch)\n",
    "        nh = _round_to_multiple(h, patch)\n",
    "    return img.resize((nw, nh), Image.Resampling.LANCZOS)\n",
    "\n",
    "def enhance_doc(img: Image.Image) -> Image.Image:\n",
    "    g = ImageOps.grayscale(img)\n",
    "    g = ImageOps.autocontrast(g)\n",
    "    g = g.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "    return g.convert(\"RGB\")\n",
    "\n",
    "def is_base64_like(s: str) -> bool:\n",
    "    if not isinstance(s, str): return False\n",
    "    if s.startswith(\"data:image/\"): return True\n",
    "    return bool(re.match(r\"^[A-Za-z0-9+/=\\s]{50,}$\", s))\n",
    "\n",
    "def load_image_raw_base64(inp: str):\n",
    "    try:\n",
    "        data = inp.split(\",\", 1)[1] if inp.startswith(\"data:image/\") else inp\n",
    "        raw  = base64.b64decode(data, validate=False)\n",
    "        return Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def make_doc_views_from_base64(inp: str):\n",
    "    if not is_base64_like(inp): return None\n",
    "    img = load_image_raw_base64(inp)\n",
    "    if img is None: return None\n",
    "    base = enhance_doc(img)\n",
    "    full = _resize_to_grid(base, DOC_MAX_EDGE, DOC_PATCH)\n",
    "    return [full]\n",
    "\n",
    "# =========================\n",
    "# ‚öôÔ∏è Í∞ïÍ±¥Ìïú HTTP Î°úÎçî (Ïû¨ÏãúÎèÑ‚Üì, ÌÉÄÏûÑÏïÑÏõÉ‚Üì)\n",
    "# =========================\n",
    "_RETRY = Retry(\n",
    "    total=1, backoff_factor=0.4,\n",
    "    status_forcelist=(429,500,502,503,504),\n",
    "    allowed_methods=frozenset([\"GET\",\"HEAD\"])\n",
    ")\n",
    "_SESSION = requests.Session()\n",
    "_SESSION.mount(\"https://\", HTTPAdapter(max_retries=_RETRY))\n",
    "_SESSION.mount(\"http://\",  HTTPAdapter(max_retries=_RETRY))\n",
    "_DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                   \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"),\n",
    "    \"Accept\": \"image/webp,image/*;q=0.8,*/*;q=0.5\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,ko;q=0.8\",\n",
    "}\n",
    "def _origin(url: str) -> str:\n",
    "    u = urlparse(url)\n",
    "    return f\"{u.scheme}://{u.netloc}\"\n",
    "def _looks_like_image_bytes(head: bytes) -> bool:\n",
    "    return head.startswith(b\"\\xff\\xd8\") or head.startswith(b\"\\x89PNG\") or head.startswith(b\"RIFF\")\n",
    "\n",
    "def load_image_resized(inp, max_edge=MAX_IMAGE_EDGE, patch=14, timeout_s=8):\n",
    "    if isinstance(inp, str) and inp.startswith((\"http://\",\"https://\")):\n",
    "        path=_cache_path(inp, max_edge)\n",
    "        if os.path.exists(path):\n",
    "            try: return Image.open(path).convert(\"RGB\")\n",
    "            except Exception: pass\n",
    "        headers = dict(_DEFAULT_HEADERS); headers[\"Referer\"] = _origin(inp)\n",
    "        try:\n",
    "            resp = _SESSION.get(inp, headers=headers, timeout=timeout_s, allow_redirects=True, stream=True)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"[HTTP] status={resp.status_code} url={inp}\")\n",
    "                return None\n",
    "            ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            content = resp.content\n",
    "            if \"image\" not in ctype and not _looks_like_image_bytes(content[:16]):\n",
    "                print(f\"[HTTP] non-image content-type={ctype} url={inp}\")\n",
    "                return None\n",
    "            img = Image.open(io.BytesIO(content))\n",
    "            if img.mode not in (\"RGB\",\"RGBA\"):\n",
    "                img = img.convert(\"RGB\")\n",
    "            elif img.mode == \"RGBA\":\n",
    "                img = img.convert(\"RGB\")\n",
    "            if APPLY_DOC_ENHANCE_HTTP:\n",
    "                try: img = enhance_doc(img)\n",
    "                except Exception: pass\n",
    "        except Exception as e:\n",
    "            print(f\"[HTTP] fetch failed: {e} url={inp}\")\n",
    "            return None\n",
    "\n",
    "        w,h=img.size\n",
    "        if max(w,h)>max_edge:\n",
    "            scale=max_edge/float(max(w, h))\n",
    "            new_w=_round_to_multiple(int(w*scale),patch)\n",
    "            new_h=_round_to_multiple(int(h*scale),patch)\n",
    "            img=img.resize((max(patch,new_w),max(patch,new_h)), Image.Resampling.LANCZOS)\n",
    "        else:\n",
    "            img=img.resize((_round_to_multiple(w,patch),_round_to_multiple(h,patch)), Image.Resampling.LANCZOS)\n",
    "        try: img.save(path, format=\"JPEG\", quality=92)\n",
    "        except Exception: pass\n",
    "        return img\n",
    "\n",
    "    # base64\n",
    "    if isinstance(inp, str):\n",
    "        try:\n",
    "            data = inp.split(\",\", 1)[1] if inp.startswith(\"data:image/\") else inp\n",
    "            if re.match(r\"^[A-Za-z0-9+/=\\s]{100,}$\", data or \"\"):\n",
    "                raw = base64.b64decode(data, validate=False)\n",
    "                img = Image.open(io.BytesIO(raw)).convert(\"RGB\")\n",
    "                w, h = img.size\n",
    "                if max(w, h) > max_edge:\n",
    "                    scale = max_edge / float(max(w, h))\n",
    "                    new_w = max(patch, _round_to_multiple(int(w * scale), patch))\n",
    "                    new_h = max(patch, _round_to_multiple(int(h * scale), patch))\n",
    "                    img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "                else:\n",
    "                    img = img.resize((_round_to_multiple(w, patch), _round_to_multiple(h, patch)), Image.Resampling.LANCZOS)\n",
    "                return img\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# ‚úÖ ÌÉúÏä§ÌÅ¨ ÎùºÏö∞ÌåÖ(ÎùºÎ≤® ÎØ∏ÏÇ¨Ïö©, Í≤∞Ï†ï Í∑úÏπôÎßå)\n",
    "# -----------------------------\n",
    "def detect_input_kind(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"unknown\"\n",
    "    s = s.strip()\n",
    "    if s.startswith((\"http://\",\"https://\")): return \"image\"\n",
    "    if re.match(r\"^data:image/\", s) or re.match(r\"^[A-Za-z0-9+/=\\s]{100,}$\", s or \"\"):\n",
    "        return \"image\"\n",
    "    return \"text\"\n",
    "\n",
    "def determine_task(input_type: str, context: str, question: str) -> str:\n",
    "    it = (input_type or \"\").strip().lower()\n",
    "    ctx = (context or \"\").strip()\n",
    "    q  = (\"\" if question is None else str(question)).strip()\n",
    "\n",
    "    if it not in (\"image\",\"text\"):\n",
    "        it = detect_input_kind(context)\n",
    "\n",
    "    if it == \"image\":\n",
    "        return \"vqa\" if q else \"captioning\"\n",
    "\n",
    "    if it == \"text\":\n",
    "        if q: return \"text_qa\"\n",
    "        if ctx.endswith(\"?\"): return \"math\" \n",
    "        return \"summarization\"\n",
    "\n",
    "    return \"summarization\"\n",
    "\n",
    "# -----------------------------\n",
    "# ÏûÖÎ†• Í∏∞Î∞ò ÌîÑÎ°¨ÌîÑÌä∏\n",
    "# -----------------------------\n",
    "def build_text_prompt(input_type: str, context: str, question: str):\n",
    "    itype = (input_type or \"\").strip().lower()\n",
    "    ctx = (context or \"\").strip()\n",
    "    q   = (\"\" if question is None else str(question)).strip()\n",
    "\n",
    "    task_r = determine_task(itype, ctx, q)\n",
    "\n",
    "    parts = []\n",
    "    if itype == \"text\" and ctx:\n",
    "        parts.append(ctx)\n",
    "\n",
    "    if q:\n",
    "        parts.append(\"Question: \" + q)\n",
    "    else:\n",
    "        if task_r == \"captioning\":\n",
    "            parts.append(\"Question: \" + DEFAULT_ASK_IMAGE)\n",
    "        elif task_r == \"math\":\n",
    "            parts.append(\"Question: \" + DEFAULT_ASK_MATH)\n",
    "        else:\n",
    "            parts.append(\"Question: \" + DEFAULT_ASK_TEXT)\n",
    "\n",
    "    if itype == \"image\":\n",
    "        if q:\n",
    "            parts.append(\"Constraints: Provide one concise sentence grounded only in the image.\")\n",
    "        else:\n",
    "            parts.append(\"Constraints: Provide 3‚Äì5 sentences, grounded only in the image.\")\n",
    "\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "# -----------------------------\n",
    "# ÌõÑÏ≤òÎ¶¨(ÌòïÏãù Í∞ïÏ†ú + Îπà Ï∂úÎ†• Í∞ÄÎìú \"-\")\n",
    "# -----------------------------\n",
    "def _first_sentence(s: str) -> str:\n",
    "    m = re.split(r\"(?<=[.!?])\\s+\", s.strip(), maxsplit=1)\n",
    "    return m[0] if m and m[0] else s.strip()\n",
    "\n",
    "def _strip_bullets_and_chatty(s: str) -> str:\n",
    "    lines = [re.sub(r'^\\s*(?:[-*‚Ä¢]+|\\d+\\.)\\s*', '', ln) for ln in s.splitlines()]\n",
    "    s2 = \" \".join([ln.strip() for ln in lines if ln.strip()])\n",
    "    s2 = re.sub(r\"[üëç‚ú®üí°‚úÖ‚ùóÔ∏è‚ùóÔ∏èü§îüìù]+\", \"\", s2)\n",
    "    return re.sub(r\"\\s+\", \" \", s2).strip()\n",
    "\n",
    "def postprocess_output(ans: str,\n",
    "                       question: str = None,\n",
    "                       input_type: str = None,\n",
    "                       image_loaded: bool = True,\n",
    "                       context: str = None):\n",
    "    s = (ans or \"\")\n",
    "\n",
    "    for p in BANNED_PHRASES:\n",
    "        s = s.replace(p, \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    if UNKNOWN_POLICY == \"off\":\n",
    "        if re.fullmatch(r\"(?i)\\s*unknown\\s*\", s): s = \"\"\n",
    "        s = re.sub(r\"(?i)\\bunknown\\b\", \"\", s).strip()\n",
    "\n",
    "    s = re.sub(r\"(?<=\\d),(?=\\d)\", \"\", s)\n",
    "\n",
    "    it = (input_type or \"\").strip().lower()\n",
    "    q  = (question or \"\").strip()\n",
    "    ctx = (context or \"\").strip()\n",
    "    task_r = determine_task(it, ctx, q)\n",
    "\n",
    "    if task_r == \"math\" and it != \"image\":\n",
    "        if \"####\" not in s:\n",
    "            nums = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s)\n",
    "            tail = nums[-1] if nums else (s.split()[-1] if s else \"\")\n",
    "            s = f\"{s}\\n#### {tail}\".strip()\n",
    "\n",
    "    if it == \"image\":\n",
    "        s = re.sub(r\"\\s*####\\s*.*$\", \"\", s).strip()\n",
    "        if q and VQA_SHORTEN_ONE_SENTENCE:\n",
    "            s = _first_sentence(s)\n",
    "\n",
    "    if task_r == \"summarization\":\n",
    "        s = _strip_bullets_and_chatty(s)\n",
    "\n",
    "    if not s.strip():\n",
    "        s = \"-\"\n",
    "\n",
    "    return s\n",
    "\n",
    "# -----------------------------\n",
    "# ‚õî Î¨∏Ïû• Í∞úÏàò ÏÉÅÌïú Ïä§ÌÜ± Í∏∞Ï§Ä\n",
    "# -----------------------------\n",
    "class StopOnSentenceCount(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, start_pos: int, max_sentences: int):\n",
    "        self.tok = tokenizer\n",
    "        self.start_pos = int(start_pos)\n",
    "        self.max_sentences = int(max_sentences)\n",
    "        self._pat = re.compile(r'[.!?](?=\\s|$|[\"‚Äù])')\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        new_ids = input_ids[0][self.start_pos:]\n",
    "        if new_ids.numel() <= 0:\n",
    "            return False\n",
    "        text = self.tok.decode(new_ids, skip_special_tokens=True)\n",
    "        ends = self._pat.findall(text)\n",
    "        return len(ends) >= self.max_sentences\n",
    "\n",
    "# -----------------------------\n",
    "# Î™®Îç∏/ÌîÑÎ°úÏÑ∏ÏÑú (QLoRA 4bit)\n",
    "# -----------------------------\n",
    "bnb = None\n",
    "if USE_4BIT:\n",
    "    try:\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes ÏÑ§Ï†ï Ïã§Ìå® ‚Üí 4bit ÎπÑÌôúÏÑ±:\", e)\n",
    "        USE_4BIT = False\n",
    "        bnb = None\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb,\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True, token=token)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ÎπÑÏ†Ñ ÌÉÄÏõå ÎèôÍ≤∞\n",
    "for n, p in model.named_parameters():\n",
    "    if any(k in n for k in [\"vision\",\"vision_tower\",\"clip\",\"image\",\"vision_model\"]):\n",
    "        p.requires_grad = False\n",
    "\n",
    "# LoRA\n",
    "lora = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora)\n",
    "\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False  # gradient_checkpointing Ìò∏Ìôò\n",
    "\n",
    "# trainable % Ï∂úÎ†•\n",
    "def count_trainable(m):\n",
    "    t=a=0\n",
    "    for p in m.parameters():\n",
    "        a+=p.numel()\n",
    "        if p.requires_grad: t+=p.numel()\n",
    "    return t,a\n",
    "t,a = count_trainable(model)\n",
    "print(f\"trainable params: {t:,} || all params: {a:,} || trainable%: {t/a*100:.4f}\")\n",
    "print(\"Î™®Îç∏ + LoRA Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "\n",
    "# -----------------------------\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú(+ÌÅ¥Î¶∞) ‚Üí ÏÑúÎ∏åÏÖã ‚Üí 90/10 Î∂ÑÌï†\n",
    "# -----------------------------\n",
    "df_full = safe_read_df(CSV_TRAIN).copy()\n",
    "for col in [\"input_type\",\"input\",\"question\",\"output\",\"task\"]:\n",
    "    if col not in df_full.columns:\n",
    "        df_full[col] = \"\" if col!=\"task\" else \"unknown\"\n",
    "df_full = df_full[~df_full[\"output\"].isna()]\n",
    "df_full = df_full[df_full[\"output\"].astype(str).str.strip().str.len()>0].reset_index(drop=True)\n",
    "df_full[\"_orig_idx\"] = np.arange(len(df_full))\n",
    "\n",
    "def stratified_or_random_sample(df, n=8000, by_cols=None, seed=42):\n",
    "    n=min(n,len(df))\n",
    "    if n<=0: return df.head(0).copy()\n",
    "    if by_cols:\n",
    "        tot=len(df); parts=[]; remain=n\n",
    "        for _,g in df.groupby(by_cols, dropna=False, group_keys=False):\n",
    "            alloc=max(1,int(round(len(g)/tot*n))); take=min(alloc,len(g))\n",
    "            parts.append(g.sample(n=take, random_state=seed)); remain-=take\n",
    "        out=pd.concat(parts) if parts else df.head(0)\n",
    "        if remain>0:\n",
    "            rest=df.drop(out.index, errors=\"ignore\")\n",
    "            if len(rest)>0: out=pd.concat([out, rest.sample(n=min(remain,len(rest)), random_state=seed)])\n",
    "        if len(out)>n: out=out.sample(n=n, random_state=seed)\n",
    "        return out.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        return df.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "def stratified_train_val_split(df, val_frac=0.1, by_cols=None, seed=42):\n",
    "    df=df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    if not by_cols:\n",
    "        n_val=max(1,int(round(len(df)*val_frac)))\n",
    "        return df.iloc[n_val:].reset_index(drop=True), df.iloc[:n_val].reset_index(drop=True)\n",
    "    parts_tr,parts_val=[],[]\n",
    "    for _,g in df.groupby(by_cols, dropna=False, group_keys=False):\n",
    "        if len(g)==1: parts_tr.append(g); continue\n",
    "        n_val=max(1,int(round(len(g)*val_frac))); g=g.sample(frac=1.0, random_state=seed)\n",
    "        parts_val.append(g.iloc[:n_val]); parts_tr.append(g.iloc[n_val:])\n",
    "    tr=pd.concat(parts_tr).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    va=pd.concat(parts_val).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return tr, va\n",
    "\n",
    "if USE_TRAIN_SUBSET:\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    if os.path.exists(SUBSET_IDX_PATH):\n",
    "        try:\n",
    "            idx_df = pd.read_csv(SUBSET_IDX_PATH)\n",
    "            keep_idx = idx_df[\"_orig_idx\"].astype(int).tolist()\n",
    "            df_core = df_full.iloc[keep_idx].reset_index(drop=True)\n",
    "            print(f\"‚òÖ Î∂ÄÎ∂Ñ ÌïôÏäµ Î™®Îìú(Ïû¨Í∞ú): subset size = {len(df_core)} (from {len(df_full)})\")\n",
    "        except Exception as e:\n",
    "            print(\"Î∂ÄÎ∂Ñ Ïù∏Îç±Ïä§ Î°úÎìú Ïã§Ìå®, ÏÉàÎ°ú ÏÉòÌîåÎßÅ:\", e)\n",
    "            by_cols=[c for c in SUBSET_GROUP_CANDIDATES if c in df_full.columns]\n",
    "            df_core = stratified_or_random_sample(df_full, n=SUBSET_SIZE, by_cols=by_cols, seed=SUBSET_SEED)\n",
    "            pd.DataFrame({\"_orig_idx\": df_core[\"_orig_idx\"].tolist()}).to_csv(SUBSET_IDX_PATH, index=False)\n",
    "            print(f\"‚òÖ Î∂ÄÎ∂Ñ ÌïôÏäµ Î™®Îìú(Ïã†Í∑ú): subset size = {len(df_core)} ‚Üí Ïù∏Îç±Ïä§ Ï†ÄÏû•\")\n",
    "    else:\n",
    "        by_cols=[c for c in SUBSET_GROUP_CANDIDATES if c in df_full.columns]\n",
    "        df_core = stratified_or_random_sample(df_full, n=SUBSET_SIZE, by_cols=by_cols, seed=SUBSET_SEED)\n",
    "        pd.DataFrame({\"_orig_idx\": df_core[\"_orig_idx\"].tolist()}).to_csv(SUBSET_IDX_PATH, index=False)\n",
    "        print(f\"‚òÖ Î∂ÄÎ∂Ñ ÌïôÏäµ Î™®Îìú(Ïã†Í∑ú): subset size = {len(df_core)} ‚Üí Ïù∏Îç±Ïä§ Ï†ÄÏû•\")\n",
    "else:\n",
    "    df_core = df_full.reset_index(drop=True)\n",
    "\n",
    "by_cols_for_split=[c for c in VAL_GROUP_CANDIDATES if c in df_core.columns]\n",
    "print(\"Ï∏µÌôî Î∂ÑÌï† Í∏∞Ï§Ä:\", by_cols_for_split if by_cols_for_split else \"ÏóÜÏùå(ÎûúÎç§)\")\n",
    "\n",
    "df_tr, df_va = stratified_train_val_split(df_core, val_frac=VAL_FRACTION, by_cols=by_cols_for_split, seed=SUBSET_SEED)\n",
    "\n",
    "pd.DataFrame({\"_orig_idx\": df_tr[\"_orig_idx\"]}).to_csv(os.path.join(SAVE_DIR, \"train_split_indices.csv\"), index=False)\n",
    "pd.DataFrame({\"_orig_idx\": df_va[\"_orig_idx\"]}).to_csv(os.path.join(SAVE_DIR, \"val_split_indices.csv\"), index=False)\n",
    "for x in (df_tr, df_va):\n",
    "    if \"_orig_idx\" in x.columns: x.drop(columns=[\"_orig_idx\"], inplace=True)\n",
    "\n",
    "print(f\"final train size: {len(df_tr)} | final val size: {len(df_va)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã/ÏΩúÎ†àÏù¥Ìä∏\n",
    "# -----------------------------\n",
    "class MultiModalDataset(TorchDataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df=df.reset_index(drop=True)\n",
    "        self.processor=processor\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row=self.df.iloc[idx]\n",
    "        input_type=row.get(\"input_type\",\"\")\n",
    "        question=row.get(\"question\",\"\")\n",
    "        context=row.get(\"input\",\"\")\n",
    "        answer=row.get(\"output\",\"\")\n",
    "\n",
    "        user_text = build_text_prompt(input_type, context, question)\n",
    "        # ÌïôÏäµ ÏãúÏóêÎäî Ïù¥ÎØ∏ÏßÄ Î°úÎî© Ïú†ÏßÄ(Ï†ïÌôïÎèÑ)\n",
    "        image = load_image_resized(context) if input_type==\"image\" else None\n",
    "\n",
    "        if image is not None and input_type==\"image\":\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "                {\"role\":\"user\",\"content\":[{\"type\":\"image\"},{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "            ]; images=[image]\n",
    "        else:\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "                {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "            ]; images=None\n",
    "\n",
    "        prompt=self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        enc=self.processor(text=prompt, images=images, return_tensors=\"pt\", padding=False)\n",
    "\n",
    "        # ‚úÖ ÌïôÏäµ ÌÜ†ÌÅ∞ ÌÅ¥Î¶Ω: ÌîÑÎ°¨ÌîÑÌä∏ Î≥¥Ï°¥, Ï†ïÎãµÎßå ÏòàÏÇ∞ ÎÇ¥\n",
    "        enc_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        enc_att = enc[\"attention_mask\"].squeeze(0)\n",
    "        ans_ids = self.processor.tokenizer(\n",
    "            str(answer) if isinstance(answer,str) else \"\",\n",
    "            return_tensors=\"pt\", add_special_tokens=True, padding=False\n",
    "        ).input_ids.squeeze(0)\n",
    "        allow = max(0, MAX_TEXT_TOKENS - enc_ids.size(0))\n",
    "        if ans_ids.size(0) > allow:\n",
    "            ans_ids = ans_ids[:allow]\n",
    "\n",
    "        in_ids  = torch.cat([enc_ids, ans_ids], dim=0)\n",
    "        attn    = torch.cat([enc_att, torch.ones_like(ans_ids)], dim=0)\n",
    "\n",
    "        prompt_len = enc_ids.size(0)\n",
    "        labels = in_ids.clone()\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        item={\"input_ids\":in_ids.long(),\"attention_mask\":attn.long(),\"labels\":labels.long()}\n",
    "        if \"pixel_values\" in enc: item[\"pixel_values\"]=enc[\"pixel_values\"].squeeze(0)\n",
    "        if \"image_grid_thw\" in enc: item[\"image_grid_thw\"]=enc[\"image_grid_thw\"].squeeze(0)\n",
    "        return item\n",
    "\n",
    "def collate(batch):\n",
    "    pad_id = processor.tokenizer.pad_token_id or 0\n",
    "    def pad_1d(tensors, pad_val, dtype=torch.long):\n",
    "        m = max(t.size(0) for t in tensors); out=[]\n",
    "        for t in tensors:\n",
    "            if t.size(0)<m:\n",
    "                pad=torch.full((m-t.size(0),), pad_val, dtype=t.dtype)\n",
    "                out.append(torch.cat([t,pad], dim=0))\n",
    "            else: out.append(t)\n",
    "        return torch.stack(out, dim=0).to(dtype)\n",
    "    b={}\n",
    "    b[\"input_ids\"]      = pad_1d([x[\"input_ids\"] for x in batch], pad_id, torch.long)\n",
    "    b[\"attention_mask\"] = pad_1d([x[\"attention_mask\"] for x in batch], 0, torch.long)\n",
    "    b[\"labels\"]         = pad_1d([x[\"labels\"] for x in batch], -100, torch.long)\n",
    "    if all(\"pixel_values\" in x for x in batch):\n",
    "        b[\"pixel_values\"]=torch.stack([x[\"pixel_values\"] for x in batch], dim=0)\n",
    "    if all(\"image_grid_thw\" in x for x in batch):\n",
    "        grids=[x[\"image_grid_thw\"] for x in batch]\n",
    "        try: b[\"image_grid_thw\"]=torch.stack(grids, dim=0)\n",
    "        except Exception: b[\"image_grid_thw\"]=grids\n",
    "    return b\n",
    "\n",
    "train_ds = MultiModalDataset(df_tr, processor)\n",
    "val_ds   = MultiModalDataset(df_va, processor)\n",
    "\n",
    "# -----------------------------\n",
    "# Ïª§Ïä§ÌÖÄ ÏßÄÌëú(ÎùºÏö∞ÌåÖ Í∏∞Î∞ò)\n",
    "# -----------------------------\n",
    "_punc_tbl = str.maketrans(\"\", \"\", string.punctuation)\n",
    "def _normalize(s):\n",
    "    s = (\"\" if s is None else str(s)).strip().lower()\n",
    "    s = s.translate(_punc_tbl)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def exact_match(pred, ref): return 1.0 if _normalize(pred) == _normalize(ref) else 0.0\n",
    "\n",
    "def token_f1(pred, ref):\n",
    "    p = _normalize(pred).split(); r = _normalize(ref).split()\n",
    "    if len(p)==0 and len(r)==0: return 1.0\n",
    "    if len(p)==0 or len(r)==0:  return 0.0\n",
    "    from collections import Counter\n",
    "    pc, rc = Counter(p), Counter(r)\n",
    "    common = sum((pc & rc).values())\n",
    "    if common==0: return 0.0\n",
    "    prec = common / max(1, len(p)); rec  = common / max(1, len(r))\n",
    "    return (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "\n",
    "def lcs_len(a, b):\n",
    "    na, nb = len(a), len(b)\n",
    "    dp = [[0]*(nb+1) for _ in range(na+1)]\n",
    "    for i in range(1, na+1):\n",
    "        ai=a[i-1]; row=dp[i]; prow=dp[i-1]\n",
    "        for j in range(1, nb+1):\n",
    "            row[j]=prow[j-1]+1 if ai==b[j-1] else max(prow[j], row[j-1])\n",
    "    return dp[na][nb]\n",
    "\n",
    "def rouge_l_f1(pred, ref):\n",
    "    p = _normalize(pred).split(); r = _normalize(ref).split()\n",
    "    if len(p)==0 and len(r)==0: return 1.0\n",
    "    if len(p)==0 or len(r)==0:  return 0.0\n",
    "    l = lcs_len(p, r); prec = l / max(1, len(p)); rec  = l / max(1, len(r))\n",
    "    return (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "\n",
    "def numeric_em(pred, ref, tol=1e-3):\n",
    "    def _num(s):\n",
    "        m = re.findall(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s.replace(\",\", \"\"))\n",
    "        return m[0] if m else None\n",
    "    pn, rn = _num(pred), _num(ref)\n",
    "    if pn is None or rn is None: return exact_match(pred, ref)\n",
    "    try:\n",
    "        pn, rn = float(pn), float(rn)\n",
    "        if rn==0: return 1.0 if abs(pn-rn)<=tol else 0.0\n",
    "        return 1.0 if abs(pn-rn)/max(1e-12, abs(rn)) <= 1e-3 or abs(pn-rn)<=tol else 0.0\n",
    "    except: return exact_match(pred, ref)\n",
    "\n",
    "def score_sample_routed(task_r, pred, ref):\n",
    "    t = (task_r or \"\").strip().lower()\n",
    "    if t in [\"text_qa\", \"vqa\"]:\n",
    "        return {\"em\": exact_match(pred, ref), \"f1\": token_f1(pred, ref)}\n",
    "    elif t in [\"summarization\", \"captioning\"]:\n",
    "        return {\"rougeL\": rouge_l_f1(pred, ref)}\n",
    "    elif t in [\"math\"]:\n",
    "        return {\"em\": numeric_em(pred, ref)}\n",
    "    else:\n",
    "        return {\"f1\": token_f1(pred, ref)}\n",
    "\n",
    "def collapse_task_metric(task_r, d):\n",
    "    t = (task_r or \"\").strip().lower()\n",
    "    if t in [\"text_qa\", \"vqa\"]:\n",
    "        return d.get(\"f1\", d.get(\"em\", 0.0))\n",
    "    elif t in [\"summarization\", \"captioning\"]:\n",
    "        return d.get(\"rougeL\", 0.0)\n",
    "    elif t in [\"math\"]:\n",
    "        return d.get(\"em\", 0.0)\n",
    "    else:\n",
    "        return d.get(\"f1\", 0.0)\n",
    "\n",
    "# -----------------------------\n",
    "# ÌèâÍ∞Ä ÏÑúÎ∏åÏÖã Íµ¨ÏÑ±(ÌÉúÏä§ÌÅ¨Î≥Ñ Í∑†Ìòï ÏÉòÌîåÎßÅ)\n",
    "# -----------------------------\n",
    "def build_eval_subset(df_va, per_task=EVAL_SAMPLES_PER_TASK, max_total=EVAL_MAX_TOTAL_SAMPLES, seed=SUBSET_SEED):\n",
    "    if len(df_va)==0: return df_va\n",
    "    tmp = df_va.copy()\n",
    "    tmp[\"_routed_task\"] = tmp.apply(lambda r: determine_task(r.get(\"input_type\",\"\"), r.get(\"input\",\"\"), r.get(\"question\",\"\")), axis=1)\n",
    "    parts=[]\n",
    "    rng = np.random.RandomState(seed)\n",
    "    for t, g in tmp.groupby(\"_routed_task\", dropna=False):\n",
    "        take = min(per_task, len(g))\n",
    "        parts.append(g.sample(n=take, random_state=rng))\n",
    "    out = pd.concat(parts).sample(frac=1.0, random_state=rng).reset_index(drop=True)\n",
    "    if len(out) > max_total:\n",
    "        out = out.sample(n=max_total, random_state=rng).reset_index(drop=True)\n",
    "    return out.drop(columns=[\"_routed_task\"], errors=\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# ÏãúÍ∞Ñ Ï†úÌïú + Ï£ºÍ∏∞ Ï†ÄÏû•\n",
    "# -----------------------------\n",
    "class TimeLimitCallback(TrainerCallback):\n",
    "    def __init__(self, max_seconds, autosave_every_sec=AUTOSAVE_EVERY_MIN*60):\n",
    "        self.start = time.time()\n",
    "        self.max_seconds = max_seconds\n",
    "        self.autosave_every_sec = autosave_every_sec\n",
    "        self.last_save = self.start\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        now=time.time()\n",
    "        if now - self.last_save >= self.autosave_every_sec:\n",
    "            control.should_save = True\n",
    "            self.last_save = now\n",
    "        if now - self.start >= self.max_seconds:\n",
    "            print(\"\\n‚è∞ ÏãúÍ∞Ñ Ï†úÌïú ÎèÑÎã¨: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû• ÌõÑ ÌïôÏäµ Ï¢ÖÎ£åÌï©ÎãàÎã§.\")\n",
    "            control.should_save = True\n",
    "            control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "# -----------------------------\n",
    "# Îã®Ïàú ÏÉùÏÑ±(ÌèâÍ∞Ä/Ï∂îÎ°†Ïö©, dedup/ÏÉòÌîåÎßÅ ÏóÜÏùå)\n",
    "# -----------------------------\n",
    "def _generate_once_greedy(model, enc, max_new_tokens: int, stopping_criteria=None, eos_token_id=None, pad_token_id=None):\n",
    "    return model.generate(\n",
    "        **enc,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=pad_token_id if pad_token_id is not None else eos_token_id\n",
    "    )\n",
    "\n",
    "def _decode_new_tokens(processor, gen_ids, enc):\n",
    "    new_ids = gen_ids[0][enc[\"input_ids\"].shape[-1]:]\n",
    "    return processor.tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "def _fast_generate_row(model, processor, row, stage_name: str, *_args_ignored):\n",
    "    input_type=row.get(\"input_type\",\"\"); question=row.get(\"question\",\"\"); context=row.get(\"input\",\"\")\n",
    "    user_text = build_text_prompt(input_type, context, question)\n",
    "    routed = determine_task(input_type, context, question)\n",
    "\n",
    "    # ÌèâÍ∞Ä/Ï∂îÎ°†ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Ïä§ÌÇµ ÏòµÏÖò Î∞òÏòÅ\n",
    "    use_images = False\n",
    "    images = None\n",
    "    if input_type==\"image\":\n",
    "        if stage_name==\"eval\" and EVAL_SKIP_IMAGE_DOWNLOAD:\n",
    "            use_images = False\n",
    "        elif stage_name==\"test\" and INFER_SKIP_IMAGE_DOWNLOAD:\n",
    "            use_images = False\n",
    "        else:\n",
    "            img = make_doc_views_from_base64(context) or [load_image_resized(context)]\n",
    "            img = [x for x in img if x is not None]\n",
    "            if len(img)>0:\n",
    "                images = img\n",
    "                use_images = True\n",
    "\n",
    "    if use_images:\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[*([{\"type\":\"image\"}]*len(images)), {\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]\n",
    "    else:\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    enc = processor(text=prompt, images=(images if use_images else None), return_tensors=\"pt\", padding=False)\n",
    "    if torch.cuda.is_available(): enc = {k:v.to(model.device) for k,v in enc.items()}\n",
    "\n",
    "    # per-task ÌÜ†ÌÅ∞/Ïä§ÌÜ±\n",
    "    start_pos = enc[\"input_ids\"].shape[-1]\n",
    "    sc = StoppingCriteriaList()\n",
    "    if stage_name == \"eval\":\n",
    "        if routed == \"captioning\":\n",
    "            local_max = CAPTION_MAX_NEW_TOKENS_EVAL\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, CAPTION_MAX_SENTENCES))\n",
    "        elif routed == \"summarization\":\n",
    "            local_max = SUM_MAX_NEW_TOKENS_EVAL\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, SUM_MAX_SENTENCES))\n",
    "        elif routed == \"math\":\n",
    "            local_max = MATH_MAX_NEW_TOKENS_EVAL\n",
    "        else:\n",
    "            local_max = MAX_NEW_TOKENS_EVAL\n",
    "    else:  # test\n",
    "        if routed == \"captioning\":\n",
    "            local_max = CAPTION_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, CAPTION_MAX_SENTENCES))\n",
    "        elif routed == \"summarization\":\n",
    "            local_max = SUM_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, SUM_MAX_SENTENCES))\n",
    "        elif routed == \"math\":\n",
    "            local_max = MATH_MAX_NEW_TOKENS_TEST\n",
    "        else:\n",
    "            local_max = MAX_NEW_TOKENS_TEST\n",
    "\n",
    "    gen_ids = _generate_once_greedy(\n",
    "        model, enc, local_max,\n",
    "        stopping_criteria=sc,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    pred = _decode_new_tokens(processor, gen_ids, enc)\n",
    "    return postprocess_output(pred, question=question, input_type=input_type, image_loaded=use_images, context=context)\n",
    "\n",
    "# -----------------------------\n",
    "# ÏΩúÎ∞±: Í∞ÑÏù¥ ÌèâÍ∞Ä(ÎùºÏö∞ÌåÖ Í∏∞Ï§Ä) + ES (Îπ†Î•∏ Î™®Îìú)\n",
    "# -----------------------------\n",
    "class MetricEvalEarlyStopCallback(TrainerCallback):\n",
    "    def __init__(self, processor, eval_df, final_dir,\n",
    "                 every_steps=EVAL_EVERY_STEPS, patience=ES_PATIENCE, min_delta=ES_MIN_DELTA):\n",
    "        self.processor = processor\n",
    "        self.eval_df   = eval_df\n",
    "        self.final_dir = final_dir\n",
    "        self.every_steps = every_steps\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.bad_cnt = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _evaluate_small(self, model):\n",
    "        if len(self.eval_df)==0: return 0.0, {}\n",
    "        model.eval()\n",
    "        per_task_scores = {}\n",
    "\n",
    "        for _, row in self.eval_df.iterrows():\n",
    "            ref  = str(row.get(\"output\",\"\"))\n",
    "            tdet = determine_task(row.get(\"input_type\",\"\"), row.get(\"input\",\"\"), row.get(\"question\",\"\"))\n",
    "            try:\n",
    "                pred = _fast_generate_row(model, self.processor, row, \"eval\")\n",
    "            except Exception:\n",
    "                pred = \"-\"\n",
    "            sdict = score_sample_routed(tdet, pred, ref)\n",
    "            s     = collapse_task_metric(tdet, sdict)\n",
    "            per_task_scores.setdefault(tdet, []).append(float(s))\n",
    "\n",
    "        task_avgs = {t: (sum(v)/len(v) if len(v)>0 else 0.0) for t,v in per_task_scores.items()}\n",
    "        macro = sum(task_avgs.values())/max(1, len(task_avgs))\n",
    "        return macro, task_avgs\n",
    "\n",
    "    def _maybe_eval_and_es(self, state, control, model, tag):\n",
    "        if len(self.eval_df)==0: return control\n",
    "        macro, task_avgs = self._evaluate_small(model)\n",
    "        msg = f\"[{tag}] step={state.global_step} macro={macro:.4f} | \" + \" \".join([f\"{k}:{v:.3f}\" for k,v in sorted(task_avgs.items())])\n",
    "        print(msg)\n",
    "        try:\n",
    "            os.makedirs(self.final_dir, exist_ok=True)\n",
    "            with open(os.path.join(self.final_dir, \"last_eval_metrics.json\"), \"w\") as f:\n",
    "                json.dump({\"step\": int(state.global_step), \"macro\": float(macro), \"by_task\": {k: float(v) for k,v in task_avgs.items()}}, f)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        improved = (self.best is None) or (macro > self.best + self.min_delta)\n",
    "        if improved:\n",
    "            self.best = macro; self.bad_cnt = 0\n",
    "            try:\n",
    "                os.makedirs(self.final_dir, exist_ok=True)\n",
    "                model.save_pretrained(self.final_dir); self.processor.save_pretrained(self.final_dir)\n",
    "                print(f\"üíæ ÏÉà Î≤†Ïä§Ìä∏ Ï†ÄÏû• (macro={macro:.4f}) ‚Üí {self.final_dir}\")\n",
    "            except Exception as e:\n",
    "                print(\"Î≤†Ïä§Ìä∏ Ï†ÄÏû• Ïã§Ìå®:\", e)\n",
    "            control.should_save = True\n",
    "        else:\n",
    "            self.bad_cnt += 1\n",
    "            print(f\"‚Ü≥ Í∞úÏÑ† ÏóÜÏùå ({self.bad_cnt}/{self.patience})\")\n",
    "            if self.bad_cnt >= self.patience:\n",
    "                print(\"üõë Early Stop Î∞úÎèô (patience ÏÜåÏßÑ)\")\n",
    "                control.should_training_stop = True\n",
    "        return control\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        if model is None: return control\n",
    "        if state.global_step>0 and (state.global_step % self.every_steps == 0):\n",
    "            control = self._maybe_eval_and_es(state, control, model, tag=\"interval\")\n",
    "        return control\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        control = self._maybe_eval_and_es(state, control, model, tag=\"epoch_end\")\n",
    "        return control\n",
    "\n",
    "# -----------------------------\n",
    "# ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞ & Ìä∏Î†àÏù¥ÎÑà\n",
    "# -----------------------------\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=CKPT_DIR,\n",
    "    per_device_train_batch_size=PER_DEV_TRAIN_BS,\n",
    "    per_device_eval_batch_size=PER_DEV_EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=USE_BF16,\n",
    "    fp16=USE_FP16,\n",
    "    logging_steps=20,\n",
    "    save_steps=400,                # ÌèâÍ∞Ä Ï£ºÍ∏∞ÏôÄ Î≥¥Ï°∞\n",
    "    save_total_limit=4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    save_safetensors=True,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "# ÌèâÍ∞Ä ÏÑúÎ∏åÏÖã Íµ¨Ï∂ï (ÏÜçÎèÑ‚Üë)\n",
    "eval_subset_df = build_eval_subset(df_va, per_task=EVAL_SAMPLES_PER_TASK, max_total=EVAL_MAX_TOTAL_SAMPLES, seed=SUBSET_SEED)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,  # (HF Ìä∏Î†àÏù¥ÎÑàÏö©, Ïã§Ï†ú ÌèâÍ∞ÄÎäî Ïª§Ïä§ÌÖÄ ÏΩúÎ∞±ÏóêÏÑú eval_subset_df ÏÇ¨Ïö©)\n",
    "    data_collator=collate,\n",
    "    callbacks=[\n",
    "        TimeLimitCallback(max_seconds=int(LIMIT_HOURS*3600)),\n",
    "        MetricEvalEarlyStopCallback(\n",
    "            processor=processor,\n",
    "            eval_df=eval_subset_df,\n",
    "            final_dir=FINAL_DIR,\n",
    "            every_steps=EVAL_EVERY_STEPS,\n",
    "            patience=ES_PATIENCE,\n",
    "            min_delta=ES_MIN_DELTA,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Torch<2.6 CVE Ïö∞Ìöå: weights-only resume\n",
    "# -----------------------------\n",
    "def try_load_adapter_weights_from(path):\n",
    "    st_path = os.path.join(path, \"adapter_model.safetensors\")\n",
    "    if not os.path.exists(st_path):\n",
    "        return False\n",
    "    try:\n",
    "        sd = safe_load_file(st_path)\n",
    "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "        print(f\"weights-only resume: missing={len(missing)}, unexpected={len(unexpected)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"weights-only load Ïã§Ìå®:\", e)\n",
    "        return False\n",
    "\n",
    "def torch_version_tuple():\n",
    "    ver = torch.__version__.split('+')[0]\n",
    "    return tuple(int(x) for x in ver.split('.')[:3])\n",
    "\n",
    "last_ckpt = get_last_checkpoint(CKPT_DIR) if os.path.isdir(CKPT_DIR) else None\n",
    "can_secure_resume = torch_version_tuple() >= (2,6,0)\n",
    "\n",
    "if last_ckpt and not can_secure_resume:\n",
    "    print(f\"üîÅ Torch<{2.6}: optimizer Î≥µÏõê ÏÉùÎûµ, Í∞ÄÏ§ëÏπòÎßå Ïù¥Ïñ¥ÏÑú ÌïôÏäµ ({last_ckpt})\")\n",
    "    ok = try_load_adapter_weights_from(last_ckpt)\n",
    "    if not ok and os.path.isdir(FINAL_DIR):\n",
    "        print(\"Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú Ïã§Ìå® ‚Üí FINAL_DIRÏóêÏÑú Í∞ÄÏ§ëÏπò Î≥µÏõê ÏãúÎèÑ\")\n",
    "        ok = try_load_adapter_weights_from(FINAL_DIR)\n",
    "    last_ckpt = None\n",
    "elif last_ckpt:\n",
    "    print(f\"üîÅ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏóêÏÑú Ïû¨Í∞ú: {last_ckpt}\")\n",
    "else:\n",
    "    print(\"üöÄ ÏÉà ÌïôÏäµ ÏãúÏûë\")\n",
    "\n",
    "# -----------------------------\n",
    "# ÌïôÏäµ & Ï†ÄÏû•\n",
    "# -----------------------------\n",
    "try:\n",
    "    if RUN_TRAIN:\n",
    "        trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "    else:\n",
    "        print(\"üö´ RUN_TRAIN=False: ÌïôÏäµÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§. (Î∞îÎ°ú Ï∂îÎ°† Îã®Í≥ÑÎ°ú)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ÌïôÏäµ Ï§ë ÏòàÏô∏ Î∞úÏÉù: {e}\\nÌòÑÏû¨ ÏÉÅÌÉú Ï†ÄÏû•ÏùÑ ÏãúÎèÑÌï©ÎãàÎã§.\")\n",
    "finally:\n",
    "    os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "    try:\n",
    "        model.save_pretrained(FINAL_DIR)\n",
    "        processor.save_pretrained(FINAL_DIR)\n",
    "    except Exception as e:\n",
    "        print(\"ÏµúÏ¢Ö Ï†ÄÏû• Ï§ë Í≤ΩÍ≥†:\", e)\n",
    "    print(\"Î™®Îç∏/ÌîÑÎ°úÏÑ∏ÏÑú Ï†ÄÏû• ÏôÑÎ£å ‚Üí\", FINAL_DIR)\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# ÌÖåÏä§Ìä∏ÏÖã Ï∂îÎ°† ‚Üí submission.csv (Î∂ÄÎ∂Ñ Ï†ÄÏû•/Ïû¨Í∞ú)\n",
    "# -----------------------------\n",
    "print(\"Generating submission.csv from test set...\")\n",
    "model.eval(); torch.set_grad_enabled(False)\n",
    "df_test = safe_read_df(CSV_TEST).reset_index(drop=True)\n",
    "\n",
    "def build_messages_for_infer(row):\n",
    "    input_type = row.get(\"input_type\",\"\")\n",
    "    question   = row.get(\"question\",\"\")\n",
    "    context    = row.get(\"input\",\"\")\n",
    "    user_text  = build_text_prompt(input_type, context, question)\n",
    "    routed     = determine_task(input_type, context, question)\n",
    "\n",
    "    use_images = False\n",
    "    images = None\n",
    "    if input_type==\"image\":\n",
    "        if INFER_SKIP_IMAGE_DOWNLOAD:\n",
    "            use_images = False\n",
    "        else:\n",
    "            img = make_doc_views_from_base64(context) or [load_image_resized(context)]\n",
    "            img = [x for x in img if x is not None]\n",
    "            if len(img)>0:\n",
    "                images = img\n",
    "                use_images = True\n",
    "\n",
    "    if use_images:\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[*([{\"type\":\"image\"}]*len(images)), {\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]; return messages, images\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INST}]},\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user_text or \"\"}]}\n",
    "        ]; return messages, None\n",
    "\n",
    "# Î∂ÄÎ∂Ñ Ï†ÄÏû• Ïù¥Ïñ¥Î∞õÍ∏∞\n",
    "processed={}\n",
    "if os.path.exists(SUBMIT_PART):\n",
    "    try:\n",
    "        dfp=pd.read_csv(SUBMIT_PART)\n",
    "        for _, r in dfp.iterrows(): processed[int(r[\"id\"])]=str(r[\"output\"])\n",
    "        print(f\"üîÅ Í∏∞Ï°¥ Î∂ÄÎ∂Ñ Í≤∞Í≥º Î°úÎìú: {len(processed)}Í∞ú\")\n",
    "    except Exception as e:\n",
    "        print(\"Î∂ÄÎ∂Ñ Í≤∞Í≥º Î°úÎìú Ïã§Ìå®, ÏÉàÎ°ú ÏãúÏûë:\", e)\n",
    "\n",
    "preds_map=dict(processed)\n",
    "elapsed_total=time.time()-GLOBAL_START\n",
    "limit_sec=max(0,int(LIMIT_HOURS*3600)-int(elapsed_total))\n",
    "infer_start=time.time()\n",
    "def time_left_ok(): return (time.time()-infer_start) < max(0, limit_sec-5*60)\n",
    "\n",
    "todo_ids=[i for i in range(len(df_test)) if i not in preds_map]\n",
    "for idx, i in enumerate(todo_ids, 1):\n",
    "    if not time_left_ok():\n",
    "        print(\"‚è∞ Ï∂îÎ°† ÏãúÍ∞Ñ ÌïúÍ≥Ñ ÎèÑÎã¨. Î∂ÄÎ∂Ñ Í≤∞Í≥º Ï†ÄÏû• ÌõÑ Ï¢ÖÎ£å.\"); break\n",
    "    row=df_test.iloc[i]\n",
    "    try:\n",
    "        messages, images = build_messages_for_infer(row)\n",
    "        prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        enc = processor(text=prompt, images=images, return_tensors=\"pt\", padding=False)\n",
    "        if torch.cuda.is_available(): enc = {k:v.to(model.device) for k,v in enc.items()}\n",
    "\n",
    "        tdet = determine_task(row.get(\"input_type\",\"\"), row.get(\"input\",\"\"), row.get(\"question\",\"\"))\n",
    "\n",
    "        # per-task ÌÜ†ÌÅ∞/Ïä§ÌÜ±\n",
    "        start_pos = enc[\"input_ids\"].shape[-1]\n",
    "        sc = StoppingCriteriaList()\n",
    "        if tdet == \"captioning\":\n",
    "            local_max = CAPTION_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, CAPTION_MAX_SENTENCES))\n",
    "        elif tdet == \"summarization\":\n",
    "            local_max = SUM_MAX_NEW_TOKENS_TEST\n",
    "            sc.append(StopOnSentenceCount(processor.tokenizer, start_pos, SUM_MAX_SENTENCES))\n",
    "        elif tdet == \"math\":\n",
    "            local_max = MATH_MAX_NEW_TOKENS_TEST\n",
    "        else:\n",
    "            local_max = MAX_NEW_TOKENS_TEST\n",
    "\n",
    "        # Ï∂îÎ°†ÎèÑ greedy + Ïä§ÌÜ± Í∏∞Ï§Ä\n",
    "        gen_ids = _generate_once_greedy(\n",
    "            model, enc, local_max,\n",
    "            stopping_criteria=sc,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        pred = _decode_new_tokens(processor, gen_ids, enc)\n",
    "        pred = postprocess_output(pred,\n",
    "                                  question=row.get(\"question\",\"\"),\n",
    "                                  input_type=row.get(\"input_type\",\"\"),\n",
    "                                  image_loaded=(images is not None),\n",
    "                                  context=row.get(\"input\",\"\"))\n",
    "        if not pred.strip():\n",
    "            pred = \"-\"\n",
    "    except Exception:\n",
    "        pred = \"-\"\n",
    "\n",
    "    preds_map[i] = pred\n",
    "\n",
    "    if (idx % INFER_SAVE_EVERY == 0) or (idx == len(todo_ids)):\n",
    "        tmp = pd.DataFrame({\"id\": sorted(preds_map.keys()),\n",
    "                            \"output\": [preds_map[k] for k in sorted(preds_map.keys())]})\n",
    "        tmp[\"output\"] = tmp[\"output\"].apply(lambda x: x if str(x).strip() else \"-\")\n",
    "        tmp.to_csv(SUBMIT_PART, index=False, encoding=\"utf-8\")\n",
    "        print(f\"[{idx}/{len(todo_ids)}] Î∂ÄÎ∂Ñ Ï†ÄÏû• ‚Üí {SUBMIT_PART}\")\n",
    "\n",
    "all_ids = list(range(len(df_test)))\n",
    "outputs = [preds_map.get(i, \"-\") for i in all_ids]\n",
    "outputs = [o if str(o).strip() else \"-\" for o in outputs]\n",
    "pd.DataFrame({\"id\": all_ids, \"output\": outputs}).to_csv(SUBMIT_OUT, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ submission.csv saved to:\", SUBMIT_OUT)\n",
    "print(\"‚úÖ Î∂ÄÎ∂Ñ Ï†ÄÏû•Î≥∏(Î∞±ÏóÖ):\", SUBMIT_PART)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPAEWbeFYsyaU07sXzFLgUu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
